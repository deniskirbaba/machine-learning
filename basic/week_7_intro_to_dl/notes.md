# Neural Networks
Нейронные сети могут решать практически любую задачу, однако требуется их правильно применять, понимая что они могут (почти всё) и главное чего они не могут и при каких условиях.

Обычная логистическая регрессия - это типичная нейронная сеть. У нас есть линейное преобразование и некоторое нелинейное преобразование.

Нейронки - это линейные модели "на стероидах", потому что мы даём нейронным сетям возможность самостоятельно выбирать нелинейные преобразования признаков.

Предпосылки нейронок:
1. Выбирать правильное признаковое описание для задачи ML - это дело очень сложное (feature engineering), так как нет никакого чёткого алгоритма для этого.
2. Мы хотим оптимизировать функцию потерь градиентным методом - так как это быстро и эффективно (соответственно на этом шаге "отаваливаются" деревья решений, хотя они и "автоматически" делают нелинейные преобразования, однако оптимизируются "жадным" методом)

Classic ML pipeline: X -> Feature Extractor (manual) -> Classifier (diff params $\theta_1$) -> Prediction

Хочется чтобы извлечением признаков и формированием признакового описания занимался какой-то параметрический метод, по которому можно найти оптимальные параметры градиентным методом (как и в Classifier).

NN pipeline: X -> Feature Extractor (diff params $\theta_2$) -> Classifier (diff params $\theta_1$) -> Prediction

При Blending-е у нас в качестве Feature Extractor-ов выступают различные модели, которые выдают нам нелинейные преобразования признаков так, чтобы потом линейная модель Classifier могла с ними справиться.

Однако в таком случае, у нас Feature Extractor и Classifier будут разделены и обучаться отдельно, а мы хотим связть их и сделать чтобы они были дифференцируемы и обучались градинетным методом одновременно. Например:

NN example pipeline: X -> Feature Extractor (Linear model + Sigmoid) -> Logistic Regression -> Prediction

То есть мы одновременно обучаем и Feature Extractor и Logistic Regression.

Если подытожить, то вместо прямого преобразования из пространства признаков в пространство ответов - мы последовательно будем преобразовывать признаковые пространства (получая некоторые промежуточные пространства) и в конце получаем предсказания.
Этот подход логичен, так как решить задачу сходу может быть очень сложно.

Нейронные сети - последовательность дифференцируемых линейных и нелинейных преобразований. 

Функции активации:
1. Sigmoid(a) = 1/(1 + e^(-a))
2. Гиперболический тангенс(a) = tanh(a)
3. ReLu(a) = max(0, a)
4. Softplus(a) = log(1 + e^a)

# Термины
1. Слой - преобразование признакового пространства, осуществляющее переход от одного признакового пространства в другое. Бывают различных типов: полносвязный слой (dense layer) ($Wx+b$), может задаваться нелинейной функцией, например сигмоида. К слою могут быть прикреплено нелинейное преобразование - в общем может быть как угодно. Не важно сколько и каких слоёв в нейронке, важно лишь то, насколько нейронка способна решать определенные задачи и какие она имеет структурные особенности.
2. Функция активации - они как раз определяют нелинейности в слоях. Они применяются к выходу слоя. ФУНКЦИИ АКТИВАЦИИ ПРИМЕНЯЮТСЯ ПОЭЛЕМЕНТНО. 
3. Backpropogation - алгоритм обучения нейронной сети - метод взятия производной сложной функции.

# Backpropogation
Каждую функцию мы можем представить в виде графа вычислений. Тогда каждым листом будет переменная, а каждый промежуточный узел - какая-то операция. 
При вычислении значения функции мы идём в одну сторону.
Тогда, имея такое представление, для расчета производной функции по каждой переменной - будем идти в обратном порядке и находить производные. 
Вычисленные таким образом градиенты затем применяются для обновления весов параметров каждого слоя.
Вообще говоря, для каждого графа вычислений строится другой граф - вычисления производной, который затем используется для вычисления градиентов (то есть не нужно на каждой итерации заново вычислять производные). 

Под "капотом" для вычисления значения выхода нейронной сети по входу (forward pass) создаётся граф вычислений (направленный ациклический граф - directed acyclic graph), а также создаётся граф для вычисления градиентов для каждого параметра нейронной сети (backward pass).

Так как локальная производная операции "+" = 1. То этот узел просто-напросто позволяет градиенту продвигаться дальше по каждой ветви графа без изменения его значения.

Backpropagation it is just recursevily application of the chain rule backwards through the computational graph.

# Activation functions
1. Сигмоида. Область значений (0, 1) смещена относительно нуля (а в нейронках (тут много линейных моделей) требуется нормировать данные), дорого считать экспоненту.
2. Гиперболический тангенс - несмещена (область значений -1, 1), но так же дорого считать экспоненты
3. ReLU - rectified linear unit. max(0, a). Легко вычислять (как значение так и производную) (быстрее до 6 раз чем сигмоида). Выход не центрированный. Проблема нулевых градиентов при x<0 (а значит градиент не пойдет дальше аргументов, на которых x < 0). При использовании ReLU требуется вставлять много нормировочных слоев, так как у на выход нецентрированный.
4. Leaky ReLU - справа от нуля так же как и ReLU, а слева по-другому. max(0.001a, a). Убирает проблему нулевых градиентов при x<0.
5. ELU - справа от нуля также как и ReLU, а слева - экспоненциальная фуникця. Но не особо используется, так как требуется считать экспоненту
6. GELU - Gaussian error linear unit. Работет лучше ReLU, ELU. GELU ведет себя асимптотически также как и ReLU, однако различается около нуля. $GELU(a) = a P(X<=a)=x Ф(a), Ф(а) - cdf нормального распределения$. 
7. SiLU - sigmoid-weighted linear unit - похож на GELU. $SiLU(a) = a*\sigma(a)$. Использовался в RL

По факту функция активации влияет на скорость обучения. И малые измененения в них могут приводить лишь к изменению скорости обучения.

Мы хотим чтобы функции активации, как и данные были центрированы (нормированны), так как при работе с линейными моделями если у нас все данные будут положительны при подаче на вход функции активации (например ReLU), то и с нее будет выход линейный, тогда получится что в нашей модели не будет нелинейного преобразования.

Как выбирать функцию активации:
1. Если нет никаких ограничений (в том числе на область значений выходов функции активации) - использовать ReLU и нормировать данные (не только в начале но и в середине сетки)
2. Функция активации - это по факту гиперпараметр. Однако в нейронных сетях такое большое количество гиперпараметров, что их выбор происходит полу-интуитивно
3. Можно попробовать использовать Leaky ReLU, ELU, GELU, SiLU
4. tanh используется там где есть ограничения на область значений
5. сигмоида используется крайне редко, только если у нас там где-то есть бинарная классификация (вставляют как функцию активации последнего слоя)

# Проблема затухающих градиентов (Vanishing gradient)
Так как градиент - композиция частных производных на определенных слоях, то в этих частных производных есть компоненты производных от функций активаций. Сигмоида плоха тем, что имеет области насыщения (в которых производная будет 0), а также имеет максимальное значение производной = 0.25. Соответственно каждый раз когда мы ставим сигмоиду в нейронную сеть у нас градиент будет уменьшаться минимум в 4 раза. У ReLU затухание градиента будет только с одной стороны.

Проблему затухающих градиентов нельзя решить простым домножением градиента на большое число. При домножении величины (градиент) на малую величину у нас уменьшается как сам сигнал так и присутствующий в нем шум, однако при умножении на малую величину у нас разница в значениях между сигналом и шумом уменьшается, а следовательно при дальнейшем умножении мы получим очень зашумленное значение.

Есть и другая проблема - взрыв градиентов. Она происзодит когда мы получаем большие частные производные из-за этого получаем большой градиент по весам.
Эту проблему решают "сжиманием" веткора градиента до какого-то оптимального значения (Gradient Clipping).

# Gradient Optimization
Определение learning rate - это тоже поиск гиперпараметра. Его нужно выбирать по поведению функции потерь на обучении (в среднем должно выглядеть как перевернутая функция логарифма). 

Существует большое количество оптимизаторов: Momentum, AdaGRad, Adadelta, RMSprop, Adam...

## Subgradient
https://en.wikipedia.org/wiki/Subderivative#The_subgradient
Субградиент - обобщение градиента. Т.е. субградиент может быть определен для функции, которые не является дифференцируемой в некоторых точках, тогда получается что в этих точках существует такой набор векторов, в котором каждый вектор будет удовлетворять уравнению: $f(x) - f(x_0) >= v * (x - x_0)$. Такие вектора называются субградиентами в точке x_0. 

## SGD
При использовании стандартного SGD у нас градиент считается по батчу. Если же у нас размер батча достаточно мал, то получается что у нас изменение градиента будет шумным (однако это не особо проблема, так как шум у нас центрирован и в среднем градиент будет идти куда надо).

## SGD with momentum
Momentum: переиспользование градиентов с предыдущих шагов
На каждом шаге будет учитывать не только данный градиент, но и усредненные градиенты с предыдущих шагов

## SGD with Nesterov momentum
Nesterov Momentum: мы вначале шагаем вдоль накопленного градиента с прошлых шагов, а затем считаем градиент в той точке в которую попали и шагаем вдоль него

Однако и в momentum и в Nesterov momentum у нас будет бОльший расход памяти, так как нам требуется хранить еще одну величину (усредненных градиентов с прошлых шагов) для каждых параметрах во всех слоях.

## AdaGrad (Adaptive gradient)
https://optimization.cbe.cornell.edu/index.php?title=AdaGrad

AdaGrad использует различные коэффициенты для изменения градиента для разных оптимизаруемых параметров. Параметрам с большим несмещенным вторым моментом присваиваются коэффициенты меньше, чем те, у которых второй момент меньше. 
$x_{t+1} = x_t - \eta diag(\epsilon I - G)^(-1/2) g_t$

## RMSprop (Root Mean Squared Propagation)
https://optimization.cbe.cornell.edu/index.php?title=RMSProp

The RMSProp update adjusts the Adagrad method to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, RMSProp uses an exponential decay that discards history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an Adagrad with a fresh start.

Algorithm:
$$S_{t+1} = \beta S_t + (1 - \beta) (grad_{\omega} L)^2$$
$$\omega_{t+1} = \omega_t - \eta \frac{grad_{\omega} L}{\sqrt{S_t}}$$

## Adam (adaptive moment estimation)
https://optimization.cbe.cornell.edu/index.php?title=Adam
This algorithm calculates the exponential moving average of gradients and square gradients. And the parameters of β1 and β2 are used to control the decay rates of these moving averages. Adam is a combination of two gradient descent methods, Momentum, and RMSP which are explained below.

The initial value of the moving averages and beta1 and beta2 values close to 1.0 (recommended) result in a bias of moment estimates towards zero. This bias is overcome by first calculating the biased estimates before then calculating bias-corrected estimates.