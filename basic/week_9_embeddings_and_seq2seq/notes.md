# Токенизация
Последовательности обладают определенными свойствами:
1. Последовательные данные
2. Переменная длина
Например: тексты, последовательности чего-либо, временные ряды и т.д.

Бывают последовательности с конечным алфавитом (например последовательность букв) и с бесконечным (например последовательность непрерывных чисел).

Если мы говорим про последовательности с дискретным алфавитом, то наименьшим смысловым элементом последовательности называют токен.

Далеко не всегда из данных понятно, чем является токен.

В различных задачах за токен могут приниматься разные значения (слово, морфема, буква...).

То есть последовательность у нас будет представлена в виде последовательности токенов.

Контекст - внутреннее представление о той фразе которую мы услышали до текущего момента. Пусть это представление может быть представлено в виде некоторого вектора, матрицы, тензора с фиксированными размерами.

Проблема работы с данными различной длины - то что нам нужно выучить (найти) некоторое отображение из последовательности переменной длины в конечно-мерное пространство.

Если мы говорим про текстовую последовательность, то у нас каждый токен состоит из символов некоторого конечного алфавита. Тогда получается что это последовательность - это некоторая реализация категориальных величин.

Тогда самое простое такое преобразование - это OneHotEncoding + какая-то функция аггрегации (например суммирование этого переменного набора onehot векторов).

Например, самый простой подход - bag of words. Это просто счетчик числа слов в словаре, который мы строим на основе последовательности.

Однако проблема суммирования - то что у нас будет распределение данных разное для возможно одного смыслогового контеста.

Также большая проблема такого подхода в том, что мы все слова "свалили" в кучу, а порядок слов и смысловые связи между ними тоже важны.

Также проблема в том, что у нас будет огромный словарь с разреженными данными в нем.

Поэтому так работать с последовательными данными нельзя.
Так как у нас контекст следующий зависит от предыдущего.

# Рекуррентность
A limitation of Vanilla Neural Networks (and also Convolutional Networks) is that their API is too constrained: they accept a fixed-sized vector as input (e.g. an image) and produce a fixed-sized vector as output (e.g. probabilities of different classes). Not only that: These models perform this mapping using a fixed amount of computational steps (e.g. the number of layers in the model). The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both.

Каждый следующий элемент будет обрабатываться при условии что мы уже обработали все предыдущие элементы в последовательности.

То есть у нас будет текущий элемент и предыдущий контекст преобразовываться в текущий контекст каким-то образом. Где вектор контекста будет фиксированной размерности.

Самый просто вариант такого преобразования - линейное преобразование.

Такое преобразование можно назвать операцией обогащения контекста. 

h_i = W [h_{i-1}; x_i] + b (+ какая-то нелинейность)

Здесь h и x мы либо конкатенируем и используем одну матрицу преобразования, либо можем поотдельности из умножать на разные матрицы преобразования а затем складывать.

Важно, чтобы нелинейность была с ограниченным range. Так как у на при рекурсивности может получиться что значение h будет монотонно зависеть от шага. А нам важно чтобы у нас h было примерно из одного распределения.

Embedding - это информативное векторное представление.

То есть используя рекуррентню нейронную сеть мы научились кодировать нашу последовательность с помощью некоторого скрытого состояния h.

То есть получается, что число шагов в рекуррентной сети = число элементов входной последовательности (токенов).

В самой структуре языка тоже очень много полезной информации, которую можно использовать для улучшения работы модели.

Исходя из этого можно добавить задачу в модель по предсказанию следующего слова, на основе текущего контекста. А так как на следующем шаге мы видим настоящее следующее слово, то можем корректировать ответы модели для более точной работы. 

Языковая модель в общем случае - это модель, которая параметризует: p(x_{i+1} | x_1, x_2, ..., x_i).
По сути у нас модель выше параметризует: p(x_{i+1} | h_i)

То есть мы не обучаем модель предсказывать конкретное слово на основе контекста, а мы учим модель строить вероятностное распределение над всеми словами на основе текущего контекста.

Также стоит учесть, что ошибка неправильно предсказанного слова на текущем шаге намного сильнее влияет, чем ошибка на шагах ранее, так как у нас градиенты будут уменьшаться с шагами (допустим если там сигмоида)

Напомним, что наша итоговая задача - качественно представлять последовательность в виде вектора контекста. А так как у нас для конкретно нашей узкой решаемой задачи может быть мало тренировочных текстов, чтобы выучить сложную структуру языка. Мы можем сначала обучить сетку для того чтобы она поняла структуру языка и текстов на других каких-то текстах, и чтобы модель нам выдавала качественный веткор контекста, а затем уже сверху дообучить её на нашем датасете.

Таким образом у нас задача от обучения с учителем пришла к типу self-supervised learning (то есть мы придумали как нам на неразмеченных данных автоматически указать разметку для модели). То есть мы даем модели текст и она сама в нем учится предсказывать следующее слово по контексту. 

Таким образом мы можем заключить, что если наша модель будет более-менее нормально предсказывать следующее слово, то это значит что она достаточно верно улавливает контест последовательности.

То есть мы целевую задачи (например классификация текста) решаем не в лоб, а вначале строим языковую модель, чтобы она научилась понимать структуру языка и текстов и по тексту строить хороший контекст, а уже затем поверх этой модели построим классификатор, который будет по контекстному вектору классифицировтаь текст.

То есть всё развитие сейчас ИИ заключается в стремлении автоматически выучить некоторую структуру и зависимость в неразмеченных данных, так как количество неразмеченных данных намного больше чем размеченных.

И важным далее является, то что дать модели правильную архитектуру, адапатированную под определенный тип данных, чтобы она смогла выделить из них важные свойства и зависимости.

Например рекуррентная нейронная сеть хорошо адаптирована к последовательностям.

Авторегрессионная модель - модель предсказывающая что-то на основе своих же предсказаний на предыдущих шагах.

Проблемы рекуррентных моделей:
1. каждый без исключения новый токен влияет ОДИНАКОВО на контекст (может придти неинформативный токен)
2. влияние старых входов достаточно быстро угасает (так как градиенто со временем размываются) - получается, что у нас достаточно локальный контекст. Gradient signal from far away is lost because it’s much smaller than from close-by. So model weights updates will be based only on short-term effects.

RNN can be used as encoder for sequentional data (h - это некоторое промежуточное представление последовательности в виде вектора фиксированной размерности (т.н. embedding))

# LSTM - long-short term memory
Эти проблемы пытается решить LSTM.

Идея LSTM в том, что у нас как раньше его локальный контекст - вектор h, который мы обновляем с каждым новым пришедшим токеном. И есть еще дополнительно глобальный контекст - память, в которую мы записываем ТОЛЬКО полезную информацию. 

И так как в LSTM у глобального контекста (C) нет функции активации - то это поможет справиться с затуханием градиента.

Проблема определения того полезный нам пришел токен или нет - это бинарная классификация. 

Когда к нам приходит новый токен нам нужно изменить контекст в 2 операции:
1. выкинуть то что не актуально
2. добавить актуальную информацию

У нас по идее элементы вектора h - независимы, так как векторы x (токены) не зависимы, так как умножение происходит по разным строкам матрицы W и вектору x.

C - cell state, h - hidden state

1-ый шаг - forget gate. Решаем задачу классификации и выявляем какую информацию надо забыть. То есть предсказываем булеву маску (маску вероятностей - насколько важен каждый элемент) для вектора контекста, на которую потом и умножим вектор контекста. То есть с учетом нового вектора мы обновляем эту маску. Тут решается задача бинарной классификации.
f_t = sigmoid(W_f [h_{t-1}; x_t] + b_f) - маска вероятностей. То есть мы на уровне архитектуры модели дали ей возможность избавиться от чего-то домножением на мтарицу вероятностей. 

2 шаг. Рассчитываем кандидата на добавление в вектор глобального контекста: 
C^~_t = tanh(W_C [h_{t-1}; x_t] + b_C)
Этого кандидата мы затем пропускаем через вероятностную матрицу, чтобы определеить только те элементы которые важны для нашего глобального контекста:
i_t = sigmoid(W_i [h_{t-1}; x_t] + b_i)

3 шаг. Обогащение вектора глобального контекста:
C_t = f_t * C_{t-1} + i_t * C^~_t

4 шаг. Объединение вектора глобального контекста с новым пришедшим токеном, тем самым мы получим вектор краткосрочной памяти (локального контекста). То есть мы краткосрочную генерируем из долгосрочной но при условии токена.
o_t = sigmoid(W_o [h_{t-1}; x_t] + b_o)
h_t = o_t * tanh(C_t)

# ResNet - Residual networks
https://en.wikipedia.org/wiki/Residual_neural_network

В глубоких нейронных сетях очень велика проблема затухающих градиентов. С ней спрашвляются с помощью SkipConnections - позволяющие сигналу пропускать несколько слоев и поступать на вход.

Это позволило не просто прокинуть вперед сигнал пропуская некоторые слои. А важно то, что на обратном проходе позволили градиентам течь в обход преобразований тем самым несколько облегчив проблему затухания градиентов.

ResNN как раз таки и основываются на использовании этих skipconnections.

Кстати в LSTM тоже есть skipconnections это как раз таки cell state - по нему тогда отсутствует затухание градиента.

# Основная идея

Мы на уровне архитектуры можем менять поведение нашей модели

# Softmax temperature

We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc).

P_i = e^(y_i/T)/sum(e^(y_k/T)), T - temperature

The temperature is a way to control the entropy of a distribution, while preserving the relative ranks of each event.

# Practice
В NLP используются технические токены.
Например, начало строки помечается токеном <SOS> (start of sentence). А в конце <EOS>.

<EOS> токен позволяет модели самой определить конец строки и завершить генерацию. Также с помощью <EOS> можно определять конец embedding для последовательности.

Также, для того чтобы выровнять последовательности в батче, используются специальные Padding tokens (например ' ').

# Embedding
Пусть у нас есть one-hot вектор, и мы применяем линейное преобразование к нему, чтобы понизить его размерность. Тогда полученный вектор будет являться его embedding-ом. 

Однако данную операцию можно упростить, так как мы умножаем one-hot вектор на матрицу, то это эквивалентно в просто выборе строки/столбца у матрицы. (так будет намного быстрее)

В DL операция построения embedding-ов достаточно частая операция. Так как у нас обычно есть на входе категориальные переменные и мы учимся их кодировать.

То есть с использованием эмбеддингов, мы нашей модели поставили еще одну задачу в поиске кодирования наших категориальных векторов (научиться в векторном виде представлять токены) в меньшей размерности ДЛЯ МИНИМИЗАЦИИ ФУНКЦИИ ПОТЕРЬ. 

То есть тут не ставиться новая прокси-задача, например как в word2vec, где она ставится на основе локального контекста найти эффективное представления слов в виде веткоров.