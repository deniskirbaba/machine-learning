# Токенизация

Последовательности обладают следующими ключевыми свойствами:

1. **Последовательные данные** — каждый элемент зависит от предыдущего.
2. **Переменная длина** — длина последовательности может варьироваться.

Примеры таких данных: текстовые последовательности, временные ряды и другие последовательности.

Последовательности можно разделить на две категории:
- С **конечным алфавитом** (например, последовательности букв или слов).
- С **бесконечным алфавитом** (например, последовательности непрерывных чисел).

## Токены

Если мы говорим о последовательностях с дискретным алфавитом, наименьшим смысловым элементом такой последовательности называют **токеном**. 

Важно понимать, что в разных задачах токены могут определяться по-разному: это может быть предложение, фраза, слово, морфема или даже символ. Следовательно, последовательность данных представляется как последовательность токенов.

## Контекст

**Контекст** — это внутреннее представление последовательности данных, которое учитывает информацию о всех предыдущих токенах. Например, контекст можно выразить через вектор, матрицу или тензор фиксированных размеров. Контекст важен для обработки последовательностей переменной длины, так как он сохраняет накопленную информацию.

## Проблемы представления последовательностей

Задача работы с последовательностями заключается в том, чтобы построить отображение последовательности переменной длины в конечномерное пространство.

Если рассматривать текстовые последовательности, то каждый токен может быть представлен символами из конечного алфавита. Таким образом, такая последовательность является реализацией **категориальных величин**.

## One-Hot Encoding

Одним из простейших способов преобразования категориальных данных является **One-Hot Encoding** — представление токенов в виде векторов, где один элемент равен 1, а остальные — 0.

Однако, чтобы представить последовательность целиком, необходимо применить некоторую **агрегационную функцию** к набору этих векторов. Пример простейшей функции агрегации — **суммирование** One-Hot векторов.

## Bag of Words

Одним из примитивных подходов является **Bag of Words** (мешок слов) — это метод, при котором учитывается частота встречаемости слов в тексте. В этом подходе порядок слов игнорируется, а последовательность сводится к счетчику числа слов в словаре.

## Проблемы примитивных подходов

1. **Утеря порядка**: Порядок слов в последовательности важен, так как он определяет смысл. При суммировании или применении методов типа Bag of Words порядок слов теряется.
2. **Смысловые связи**: Смысловые связи между токенами (например, контекстные зависимости между словами) не учитываются, что приводит к искажению смысла.
3. **Разреженность данных**: Словари токенов могут быть очень большими, что приводит к созданию разреженных матриц, в которых много нулевых элементов. Это делает такие представления неэффективными для работы с последовательными данными.

## Заключение

Использование простых подходов вроде One-Hot Encoding и Bag of Words неприемлемо для работы с последовательными данными, поскольку не учитываются важные зависимости между элементами последовательности. Для этого требуются более сложные модели, такие как рекуррентные нейронные сети (RNN) и seq2seq модели, которые способны моделировать контекст и последовательность взаимосвязанных токенов.

# Рекуррентность

Ограничение обычных нейронных сетей (включая сверточные сети) заключается в том, что их архитектура слишком статична: они принимают фиксированный вектор на вход (например, изображение) и выдают фиксированный вектор на выход (например, вероятности различных классов). Кроме того, такие модели выполняют это преобразование за фиксированное количество шагов (например, определяемое количеством слоев модели).

Рекуррентные нейронные сети (RNN) особенно интересны тем, что позволяют работать с последовательностями данных, обеспечивая возможность моделировать зависимости как на входе, так и на выходе.

## Основная идея рекуррентности

Каждый следующий элемент последовательности обрабатывается с учетом того, что мы уже обработали все предыдущие элементы. Текущий элемент и предыдущий контекст преобразуются в новый контекст, который хранит информацию о последовательности (это называют "обогащением" контекста). Этот контекст обычно представлен вектором фиксированной размерности.

Простейший способ такого преобразования — **линейное преобразование**:

$$ h_i = W \cdot [h_{i-1}; x_i] + b $$

где $ h_i $ — это контекст на шаге $ i $, $ h_{i-1} $ — контекст на предыдущем шаге, $ x_i $ — текущий входной элемент последовательности, а $ W $ и $ b $ — параметры модели. После линейного преобразования обычно применяют **нелинейность** для получения более сложного представления.

Важно, чтобы нелинейная функция была ограниченной, иначе значение $ h_i $ может монотонно изменяться с каждым шагом, что приведет к неустойчивости. Примером таких нелинейностей являются сигмоид или tanh.

## Обогащение контекста

Операция над контекстом $ h $ — это способ обогащения представления последовательности, включающей всю предыдущую информацию. Контекст можно получить разными способами: например, конкатенировать $ h_{i-1} $ и $ x_i $, умножая на одну общую матрицу $ W $, или же умножать их на разные матрицы и затем суммировать.

## Embedding

**Embedding** — это информативное векторное представление элементов последовательности, которое служит для кодирования токенов в удобной для модели форме.

Используя рекуррентную нейронную сеть, мы кодируем всю последовательность в скрытое состояние $ h $, которое на каждом шаге содержит информацию обо всей предыдущей последовательности.

## Связь числа шагов и длины последовательности

Число шагов в рекуррентной сети совпадает с числом элементов входной последовательности (токенов). Это означает, что количество итераций, которые сеть должна выполнить, зависит от длины входных данных.

## Языковая модель

Рекуррентные сети хорошо подходят для построения **языковых моделей**. Языковая модель обучается предсказывать вероятность следующего токена на основе предыдущих:

$$ p(x_{i+1} | x_1, x_2, ..., x_i) $$

В более общем случае мы параметризуем вероятность через скрытое состояние:

$$ p(x_{i+1} | h_i) $$

Цель — не предсказать конкретное следующее слово, а построить вероятностное распределение над всеми возможными словами на основе текущего контекста.

## Проблема исчезающих градиентов

Одной из основных проблем рекуррентных сетей является **затухание градиентов**. Ошибки, возникающие на дальних шагах последовательности, слабо передаются назад при обучении, и модель склонна сосредотачиваться на более "близких" к текущему моменту данных. Это приводит к тому, что сеть плохо учитывает долгосрочные зависимости.

## Преимущества и использование

Основная задача RNN — качественно представлять последовательность в виде контекстного вектора. Однако для сложных языковых структур требуется большое количество данных, что не всегда возможно в рамках одной задачи.

Для улучшения моделей можно сначала обучить рекуррентную сеть на большом наборе текстов, чтобы она поняла структуру языка, а затем дообучить её на целевом датасете. Такой подход называется **self-supervised learning**: модель обучается на неразмеченных данных предсказывать следующее слово, а затем используется для решения целевых задач, например, классификации текста.

## Важность архитектуры

То есть всё развитие сейчас ИИ заключается в стремлении автоматически выучить некоторую структуру и зависимость в неразмеченных данных, так как количество неразмеченных данных намного больше чем размеченных.

И важным далее является, то что дать модели правильную архитектуру, адапатированную под определенный тип данных, чтобы она смогла выделить из них важные свойства и зависимости.

Например рекуррентная нейронная сеть хорошо адаптирована к последовательностям.

## Авторегрессионные модели

**Авторегрессионные модели** предсказывают следующее состояние на основе своих же предыдущих предсказаний. Это важно для обработки последовательностей, где предсказания на каждом шаге влияют на дальнейшие шаги.

## Проблемы рекуррентных моделей

1. **Одинаковое влияние всех токенов**: Каждый новый токен оказывает одинаковое влияние на контекст, даже если он неинформативен.
2. **Быстрое угасание влияния старых токенов**: Контекст теряет информацию о далёких токенах из-за затухания градиентов, что делает учёт долгосрочных зависимостей проблематичным.

## Заключение

Рекуррентные нейронные сети (RNN) можно использовать как кодировщики для последовательных данных. Они позволяют представлять последовательность в виде вектора фиксированной размерности (embedding), который содержит информацию обо всей последовательности. Однако их эффективность зависит от решения проблемы затухающих градиентов и правильной архитектуры.

# LSTM — Long Short-Term Memory

LSTM (долгосрочная краткосрочная память) разработана для решения проблем, присущих обычным рекуррентным нейронным сетям (RNN), в частности проблем затухания градиентов и невозможности захвата долгосрочных зависимостей.

### Основная идея LSTM

В LSTM используется два типа контекста:
1. **Локальный контекст** — вектор $ h $, который обновляется с каждым новым токеном, аналогично RNN.
2. **Глобальный контекст** — вектор памяти $ C $, который сохраняет только полезную информацию и обновляется выборочно.

Ключевая идея в том, что глобальный контекст позволяет избегать затухания градиентов благодаря тому, что для него нет функции активации (например, сигмоиды или tanh), что упрощает передачу информации через множество шагов.

## Проблема выбора полезной информации

Определение, является ли новый токен полезным для модели — это по сути задача бинарной классификации. При обновлении контекста решаются две задачи:
1. **Забыть** неактуальную информацию.
2. **Добавить** новую полезную информацию.

## Важные компоненты LSTM

В LSTM добавляются специальные "ворота" (gates), которые управляют процессом обновления памяти. На каждом шаге происходят следующие ключевые операции:

### 1. Forget Gate (Ворота забывания)

На первом шаге модель решает, какую часть информации из предыдущего контекста нужно "забыть". Это делается с помощью предсказания вероятностной маски для вектора контекста $ C $. 

Формула forget gate:

$$
f_t = \sigma(W_f [h_{t-1}; x_t] + b_f)
$$

Где:
- $ f_t $ — это вектор вероятностей (маска), каждая компонента которого показывает, насколько важна соответствующая часть контекста $ C_{t-1} $.
- $ \sigma $ — сигмоида, использующаяся для получения значений от 0 до 1 (вероятности).
- $ W_f $ и $ b_f $ — параметры модели.

### 2. Candidate для обновления глобального контекста

На втором шаге рассчитывается "кандидат" для обновления глобального контекста $ C $. Этот кандидат — это новая информация, которую сеть рассматривает для добавления в память. Для этого используется функция $ \tanh $, которая ограничивает значения кандидатов в диапазоне от -1 до 1.

Формула для вычисления кандидата:

$$
\tilde{C}_t = \tanh(W_C [h_{t-1}; x_t] + b_C)
$$

Затем вычисляется вектор **input gate** $ i_t $, который определяет, насколько важно добавление каждого элемента кандидата в глобальный контекст:

$$
i_t = \sigma(W_i [h_{t-1}; x_t] + b_i)
$$

### 3. Обновление глобального контекста

Третий шаг — обновление вектора глобального контекста $ C_t $. Это делается с помощью комбинации забывания старого контекста и добавления новой информации:

$$
C_t = f_t \cdot C_{t-1} + i_t \cdot \tilde{C}_t
$$

Таким образом, модель забывает ненужную информацию (через $ f_t $) и добавляет важные компоненты кандидата $ \tilde{C}_t $ с учетом сигнала $ i_t $.

### 4. Обновление краткосрочной памяти (локального контекста)

На последнем шаге происходит объединение обновленного глобального контекста с текущим входным токеном для получения вектора локальной памяти $ h_t $, который служит краткосрочным представлением состояния.

Для этого используется еще один ворота — **output gate**:

$$
o_t = \sigma(W_o [h_{t-1}; x_t] + b_o)
$$

Финальное состояние $ h_t $, которое передается на следующий шаг, рассчитывается как:

$$
h_t = o_t \cdot \tanh(C_t)
$$

Здесь $ o_t $ управляет тем, какая часть глобального контекста будет использована для генерации локального контекста.

## Заключение

LSTM — это усовершенствованная версия RNN, которая решает проблему затухающих градиентов и лучше справляется с долгосрочными зависимостями. Это достигается благодаря четырем основным операциям:
1. **Forget gate** — решает, что забыть из старого контекста.
2. **Input gate** — определяет, какая новая информация будет добавлена.
3. **Обновление глобального контекста** — комбинация забывания и добавления.
4. **Output gate** — управляет генерацией локального контекста для следующего шага.

LSTM благодаря этим механизмам способна удерживать и обновлять информацию в памяти на протяжении длительных последовательностей, что делает её подходящей для задач, где важны долгосрочные зависимости (например, машинный перевод, распознавание речи, языковое моделирование).

# GRU — Gated Recurrent Unit

**GRU (Gated Recurrent Unit)** — это разновидность рекуррентной нейронной сети (RNN), предложенная как упрощенная альтернатива LSTM (Long Short-Term Memory). GRU сохраняет многие преимущества LSTM, но имеет более простую архитектуру, что снижает вычислительные затраты и может улучшить обучение на небольших датасетах.

## Архитектура GRU

В GRU отсутствует отдельный "cell state", который присутствует в LSTM. Вместо этого, вся информация о состоянии последовательности хранится в одном векторе, который называется **hidden state** (скрытое состояние). Главные компоненты GRU — это два механизма (гейта):
1. **Update gate** 
2. **Reset gate** 

### Update Gate

**Update gate** управляет тем, какую часть предыдущей информации скрытого состояния $ h_{t-1} $ необходимо сохранить и передать дальше. Это позволяет модели контролировать, какие зависимости от предыдущих шагов нужно сохранять на текущем шаге.

Формула обновляющего гейта:

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t])
$$

Здесь:
- $ z_t $ — вектор обновляющего гейта.
- $ W_z $ — веса, обучаемые моделью.
- $ h_{t-1} $ — скрытое состояние на предыдущем шаге.
- $ x_t $ — текущий входной вектор.
- $ \sigma $ — сигмоида, которая сжимает значения в диапазон [0, 1], задавая степень "обновления" каждого элемента скрытого состояния.

### Reset Gate

**Reset gate** определяет, сколько информации из предыдущего скрытого состояния нужно забыть. Это позволяет сети сбросить ненужную информацию и учитывать только наиболее важную информацию для текущего временного шага.

Формула:

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t])
$$

Где $ r_t $ — вектор, который также вычисляется через сигмоидную функцию.

### Новое скрытое состояние

После вычисления гейтов сеть генерирует новое скрытое состояние, которое зависит от текущего входного сигнала, предыдущего состояния и гейтов. Оно вычисляется следующим образом:

$$
\tilde{h}_t = \tanh(W_h \cdot [r_t * h_{t-1}, x_t])
$$

Здесь $ r_t * h_{t-1} $ — это скрытое состояние, модифицированное сбросным гейтом. Сигнал проходит через функцию гиперболического тангенса (tanh), чтобы получить новое возможное скрытое состояние $ \tilde{h}_t $.

И, наконец, итоговое скрытое состояние на текущем шаге $ h_t $ является смесью предыдущего состояния $ h_{t-1} $ и нового состояния $ \tilde{h}_t $, контролируемой обновляющим гейтом:

$$
h_t = z_t * h_{t-1} + (1 - z_t) * \tilde{h}_t
$$

Таким образом, GRU решает задачу, как обновлять состояние, комбинируя старую и новую информацию на основе вычисленных гейтов.

## Отличия GRU от LSTM

Хотя GRU и LSTM работают по схожим принципам, GRU проще по своей структуре и имеет следующие отличия:
1. **Меньшее количество гейтов**: В GRU два гейта (reset и update), тогда как в LSTM три (input, forget и output). Это делает GRU более легковесной и быстрее в вычислениях.
2. **Отсутствие отдельного cell state**: В LSTM есть отдельный вектор для долгосрочной памяти (cell state), который контролируется отдельно от hidden state. В GRU это объединено в один скрытый вектор.
3. **Меньше параметров**: Из-за отсутствия дополнительного cell state и меньшего числа гейтов, GRU имеет меньше параметров, что может упростить обучение и снизить риск переобучения.

## Преимущества GRU

1. **Улучшение долговременной памяти**: Как и LSTM, GRU эффективно справляется с сохранением долгосрочных зависимостей в последовательных данных.
2. **Меньшая вычислительная сложность**: Благодаря более простой архитектуре по сравнению с LSTM, GRU быстрее обучается и имеет меньше параметров, что снижает требования к памяти.
3. **Хорошие результаты на небольших датасетах**: За счет своей простоты GRU может показывать лучшие результаты на небольших или менее сложных задачах по сравнению с LSTM.

## Применение GRU

GRU, как и LSTM, широко применяется в задачах, связанных с обработкой последовательных данных, таких как:
- Обработка естественного языка (NLP): перевод, текстовая генерация, машинное чтение и ответы на вопросы.
- Обработка временных рядов: прогнозирование временных рядов, обработка сигналов, финансовые данные.
- Системы рекомендаций: анализ последовательных данных о предпочтениях пользователей.

# ResNet — Residual Networks

**ResNet (Residual Networks)** — это архитектура глубоких нейронных сетей, которая решает одну из ключевых проблем при обучении очень глубоких моделей: проблему затухающих градиентов. Эта архитектура была предложена в работе "Deep Residual Learning for Image Recognition" (He et al., 2015) и позволила успешно обучать сети с сотнями и даже тысячами слоев.

## Проблема затухания градиентов

При увеличении глубины нейронной сети возникает проблема, что сигналы обратного распространения (градиенты) становятся слишком малы, чтобы эффективно обновлять веса на начальных слоях сети. Это приводит к тому, что обучение глубоких сетей становится затруднительным, и они не могут эффективно находить оптимальные параметры, так как на начальных слоях сигналы слишком сильно ослабляются.

## Skip Connections (Пропускные связи)

Решение, предложенное в ResNet, заключается в использовании **skip connections** (или shortcut connections) — пропускных связей, которые позволяют сигналу пропускать один или несколько слоев и поступать напрямую на выход, минуя промежуточные преобразования.

### Идея пропускных связей

Вместо того, чтобы каждый слой обучался преобразовывать входные данные в нечто совершенно новое, ResNet позволяет слоям корректировать результаты предыдущих слоев путем добавления пропускных связей. Это приводит к следующему выражению для обновления активаций:

$$
h(x) = F(x, W) + x
$$

Где:
- $ x $ — это входной вектор.
- $ F(x, W) $ — нелинейное преобразование входных данных с параметрами $ W $.
- $ h(x) $ — выходной вектор, который является суммой преобразованного входа $ F(x, W) $ и самого входа $ x $.

Таким образом, сеть обучается не явной функции $ h(x) $, а **остаточной** функции $ F(x, W) $, что упрощает задачу сети, так как модели легче корректировать существующие данные, чем строить новые представления "с нуля".

### Преимущества Skip Connections

1. **Обход преобразований**: Пропускные связи позволяют градиентам распространяться в обход преобразований через нелинейные слои. Это помогает бороться с затуханием градиента, так как градиенты могут напрямую передаваться через пропускные связи.
   
2. **Облегчение обучения**: Благодаря тому, что сети обучаются остаточной функции, они могут легче адаптироваться к изменениям, и ошибки легче передаются обратно на начальные слои.

3. **Стабильность обучения глубоких сетей**: Использование skip connections позволило построить сети с сотнями и даже тысячами слоев без существенных проблем с обучением.

## Архитектура ResNet

Стандартная архитектура ResNet состоит из блоков, называемых **residual блоками**, где присутствуют пропускные связи. Каждый residual блок включает в себя два или три сверточных слоя с нелинейностями и пропускную связь, которая объединяет вход блока с его выходом.

Типичный residual блок можно описать следующей схемой:
1. Входной сигнал $ x $ проходит через два сверточных слоя (включая нормализацию и ReLU).
2. После этого к результату этих слоев добавляется исходный сигнал $ x $ (skip connection).
3. Итоговый результат поступает на следующий блок.

### Углубление сети

Использование ResNet позволяет строить гораздо более глубокие сети по сравнению с традиционными CNN (Convolutional Neural Networks), где глубина ограничивается проблемами обучения. Например, в оригинальной работе авторы предложили ResNet-50, ResNet-101 и ResNet-152, где числа обозначают количество слоев в модели.

## LSTM и Skip Connections

Интересно отметить, что аналогичная идея **skip connections** присутствует и в LSTM (Long Short-Term Memory). В LSTM роль пропускных связей выполняет **cell state** (глобальный контекст). Cell state передает информацию от одного шага к другому практически без изменений, что помогает справляться с затуханием градиентов и сохранять долгосрочные зависимости.

# Основная идея

Мы на уровне архитектуры можем менять поведение нашей модели

# Softmax temperature

We can also play with the temperature of the Softmax during sampling. Decreasing the temperature from 1 to some lower number (e.g. 0.5) makes the RNN more confident, but also more conservative in its samples. Conversely, higher temperatures will give more diversity but at cost of more mistakes (e.g. spelling mistakes, etc).

$$ P_i = \frac{\exp y_i/T}{\sum_k\exp y_k/T}$$
, where $T$ - temperature

The temperature is a way to control the entropy of a distribution, while preserving the relative ranks of each event.

# Practice

В NLP используются технические токены.

Например, начало строки помечается токеном <SOS> (start of sentence). А в конце <EOS>.

<EOS> токен позволяет модели самой определить (предсказать) конец строки и завершить генерацию. Также с помощью <EOS> можно определять конец embedding для последовательности.

Также, для того чтобы выровнять последовательности в батче, используются специальные Padding tokens (например ' ').

# Embedding

**Embedding** — это техника, используемая в машинном обучении и глубоком обучении для представления категориальных переменных или дискретных объектов в виде векторов в непрерывном пространстве меньшей размерности. Этот подход позволяет моделям эффективно работать с дискретными данными, такими как слова, символы или другие категориальные сущности.

## Проблемы One-Hot Кодирования

Один из простейших способов кодирования категориальных данных — это **one-hot encoding**. В этой схеме каждому уникальному объекту (например, слову) соответствует вектор длины, равной количеству всех объектов, с единственной единицей на позиции, соответствующей объекту, и нулями на остальных позициях. Однако у этого подхода есть существенные недостатки:
- **Высокая размерность**: Вектор имеет размерность, равную количеству уникальных объектов (например, слов), что может быть очень большим.
- **Разреженность**: Вектора состоят в основном из нулей, что затрудняет обработку таких данных и увеличивает вычислительные затраты.
- **Отсутствие смысловых связей**: Вектора не несут информации о схожести объектов. Например, два похожих слова будут кодироваться совершенно разными векторами, что не отражает их взаимосвязь.

## Как работает Embedding

Вместо использования one-hot векторов, можно обучить модель представлять каждый объект (например, слово) в виде вектора меньшей размерности — **embedding**. Это делается с помощью линейного преобразования, которое проектирует one-hot вектор в пространство меньшей размерности.

Представим, что у нас есть one-hot вектор. При умножении его на матрицу весов $ W $, мы получаем вектор меньшей размерности, который и будет **embedding** для данного объекта. Например:

$$
\text{embedding}(x) = W \cdot x
$$

Где:
- $ x $ — one-hot вектор,
- $ W $ — обучаемая матрица весов,
- результат $ \text{embedding}(x) $ — это вектор embedding для объекта $ x $.

### Упрощение через индексирование

Однако эту операцию можно упростить. Поскольку one-hot вектор содержит только одну единицу на позиции, соответствующей объекту, умножение на матрицу весов эквивалентно выбору строки этой матрицы, которая соответствует позиции единицы в one-hot векторе. Это значительно ускоряет вычисления и экономит память. Таким образом, процесс обучения embeddings может быть интерпретирован как поиск наиболее подходящей строки из матрицы весов для каждого объекта.

## Embedding в глубоких нейронных сетях

**Embeddings** являются важным инструментом в задачах, связанных с категориальными данными, такими как обработка естественного языка (NLP), рекомендательные системы, анализ временных рядов и др. В таких задачах embedding вектор становится компактным и информативным представлением объекта, которое затем используется моделью для выполнения основной задачи, такой как классификация или предсказание.

При этом задача модели состоит не только в решении основной задачи (например, классификация текста), но и в обучении качественных embedding-векторов для категориальных данных. Модель учится отображать категориальные векторы в такое пространство, где объекты, имеющие похожие свойства или контекст, будут находиться ближе друг к другу.

## Embedding и Word2Vec

В некоторых случаях embedding может строиться с использованием прокси-задач. Примером является модель **Word2Vec**, где задача обучается на основе предсказания контекста слова. Основная идея заключается в том, чтобы слова, которые часто встречаются в похожих контекстах, имели близкие embedding-вектора.

В **Word2Vec** прокси-задача формулируется следующим образом: для каждого слова найти такие векторные представления, которые бы позволили предсказывать соседние слова в контексте (CBOW) или, наоборот, предсказывать текущее слово по соседним (Skip-gram).

## Преимущества Embeddings

1. **Снижение размерности**: Embeddings позволяют уменьшить размерность входных данных, что делает вычисления более эффективными.
2. **Информативные представления**: Векторы embedding содержат информацию о схожести между объектами. Слова или другие объекты, имеющие общие свойства, будут располагаться ближе друг к другу в embedding-пространстве.
3. **Гибкость**: Embeddings могут обучаться как часть более сложной модели и адаптироваться к конкретной задаче, что делает их универсальным инструментом для работы с категориальными данными.

## Применение Embeddings

Embeddings активно используются в следующих задачах:
- **Обработка естественного языка (NLP)**: Представление слов, предложений, документов в виде векторов для задач классификации, машинного перевода, генерации текста и т.д.
- **Рекомендательные системы**: Embeddings для пользователей и товаров используются для анализа предпочтений и формирования рекомендаций.
- **Генерация изображений и видео**: Embeddings применяются для кодирования признаков изображений и их преобразования в компактное представление.
- **Обработка графов**: В задачах анализа графов вершины могут быть представлены с помощью embedding-векторов, что облегчает обработку структурных данных.

## Заключение

Embedding — это мощный метод представления категориальных данных в компактной и информативной форме. Он позволяет моделям глубинного обучения эффективно обучаться и решать задачи с высокоразмерными и разреженными данными, улучшая производительность и обеспечивая лучшее понимание взаимосвязей между объектами.
