# Decision Tree (Решающее дерево)

Решающее дерево может решать задачи как регрессии, так и классификации. Оно особенно эффективно при наличии нелинейных зависимостей между признаками. Однако деревья решений склонны к переобучению, особенно при большой глубине, что может негативно сказываться на обобщающей способности модели.

На каждом шаге алгоритма происходит рекурсивное разбиение исходной выборки на две подвыборки на основе сравнения значений признаков с пороговыми значениями. Этот процесс продолжается до тех пор, пока не выполнится условие остановки (например, максимальная глубина дерева или минимальное количество элементов в листе).

## Как происходит разбиение?  
На каждом шаге решается оптимизационная задача, направленная на минимизацию функции неоднородности (гетерогенности) подвыборок после разбиения:
$$ \frac{|T|}{|Q|} H(T) + \frac{|F|}{|Q|} H(F) \rightarrow \min_{j, t} $$
где:
- $ Q $ — исходная выборка,
- $ T $ и $ F $ — подвыборки после разбиения по условию $ x_j < t $,
- $ H $ — мера неоднородности множества (иногда называется **функционал неоднородности**),
- $ j $ — индекс признака,
- $ t $ — пороговое значение для признака.

## Возможные меры неоднородности $ H $ (для классификации):

1. **Энтропия**:
   $$ H(T) = -p_0 \log(p_0) - p_1 \log(p_1) $$
   где $ p_0 $ и $ p_1 $ — вероятности объектов класса 0 и класса 1 в подвыборке $ T $.

   Энтропия используется для измерения неопределенности. В деревьях решений она оценивает, насколько однородны объекты в подвыборке. Чем больше неопределенность (или неоднородность), тем выше значение энтропии.

2. **Индекс Джини (Gini impurity)**:
   $$ H(T) = 1 - p_0^2 - p_1^2 $$

   Индекс Джини показывает вероятность того, что два случайно выбранных объекта из подвыборки принадлежат к разным классам. Чем меньше значение индекса Джини, тем более однородно множество.

Эти меры легко обобщаются на многоклассовую классификацию.

## Адаптация к задачам классификации и регрессии
Решающее дерево является универсальным алгоритмом, который может использоваться как для классификации (бинарной и многоклассовой), так и для регрессии. Разница заключается в выборе функционала неоднородности $ H $. Для классификации используются энтропия или индекс Джини, а для регрессии — минимизация дисперсии или среднего квадратичного отклонения.

## Оптимизация разбиений
Чтобы найти оптимальный порог для каждого признака, алгоритм решает задачу минимизации по всем возможным пороговым значениям признаков. Это жадный подход, так как на каждом шаге выбирается текущее наилучшее разбиение, но не обязательно глобально оптимальное.

## Предсказание
Для классификации предсказание производится путем прохода по дереву, и в конечном листе (узле) определяется вероятность принадлежности к каждому классу, вычисляемая через частоты объектов каждого класса в этом листе.

Для регрессии в каждом листе предсказывается среднее значение целевой переменной для объектов, попавших в этот лист.

## Задача регрессии
В случае задачи регрессии изменяется функционал неоднородности. Если в задаче классификации в каждом листе предсказывалась константная вероятность, то в задаче регрессии предсказывается константное значение целевой переменной.

Пример меры неоднородности для задачи регрессии:
$$ H(T) = \frac{1}{|T|} \sum_{k} \left( y^{(k)} - \bar{y} \right)^2 $$
где:
- $ y^{(k)} $ — истинное значение целевой переменной для объекта $ k $,
- $ \bar{y} $ — среднее значение целевой переменной в подвыборке $ T $.

Таким образом, в регрессии минимизируется среднеквадратичное отклонение (MSE) от среднего значения в каждом листе.

Ваши записи о методах усечения (pruning) деревьев и связанных концепциях хорошо описывают основные аспекты работы с деревьями решений. Однако можно улучшить формулировки, уточнить детали и сделать некоторые моменты более понятными. Вот исправленная и улучшенная версия:

---

## Pruning (Усечение деревьев)

Усечение деревьев (pruning) — это техника, используемая для борьбы с переобучением, которое часто возникает в полностью построенных деревьях решений. Существует два основных подхода:

1. **Pre-pruning (предварительное усечение)**  
   Это усечение происходит во время построения дерева. Оно используется на этапе обучения, чтобы предотвратить построение слишком глубокой структуры. Однако на практике предварительное усечение редко используется, так как более часто встречаются подходы, основанные на post-pruning. Механизм pre-pruning может включать раннюю остановку разбиения на основе определённых критериев, таких как минимальный прирост информации.

2. **Post-pruning (последующее усечение)**  
   Последующее усечение происходит после того, как дерево уже построено. В этом случае на уровне листьев происходит обратное объединение ветвей на основе какого-либо критерия. Это может быть выполнено с использованием жадного алгоритма, который оценивает качество слияния поддеревьев (например, по приросту информативности или увеличению ошибки).

Усечение деревьев помогает уменьшить переобучение, упрощая модель и повышая её обобщающую способность.

### Применение усечения в других алгоритмах
Аналогичные идеи используются и в других моделях, например, в нейронных сетях, где можно "усекать" параметры (например, через dropout) для улучшения обобщающей способности модели.

## Ограничения и гиперпараметры деревьев

Так как дерево решений — это кусочно-постоянная функция, оптимизация его параметров с использованием градиентных методов (как в случае с линейными моделями или нейронными сетями) не имеет смысла. Вместо этого дерево строится с использованием жадных алгоритмов.

Количество параметров дерева (например, признаки и пороговые значения) увеличивается с ростом глубины дерева, что приводит к риску переобучения. Для управления сложностью дерева используются следующие гиперпараметры:

- **Критерий остановки**. Можно строить дерево до тех пор, пока не исчерпается вся информация (например, энтропия становится равной нулю), но это приведет к сильному переобучению. Более разумный подход — остановить разбиение, когда прирост информативности становится меньше определенного порога.
  
- **Максимальная глубина дерева**. Ограничение глубины дерева предотвращает его чрезмерный рост и переобучение.

- **Минимальный размер листа**. Определяет минимальное количество объектов, которое должно быть в каждом листе. Это помогает избегать создания слишком маленьких листьев с высокими вариациями.

## Ансамбли деревьев

Отдельные деревья решений редко используются в одиночку, так как их склонность к переобучению делает их менее эффективными. Однако деревья часто применяются в виде ансамблей, таких как случайные леса (Random Forest) или градиентный бустинг деревьев (Gradient Boosting Trees), что позволяет значительно повысить точность и устойчивость модели.

## Работа с пропущенными данными

Решающее дерево хорошо справляется с пропущенными значениями в данных. Если при классификации или регрессии встречается объект с пропущенным значением признака, то можно пойти сразу по обеим ветвям разбиения и вычислить результат, используя вероятности для выбора каждого поддерева. Это можно выразить следующим образом:
$$ y^* = \frac{|T|}{|Q|} \cdot y^*_T + \frac{|F|}{|Q|} \cdot y^*_F $$  
где $ y^*_T $ и $ y^*_F $ — предсказания для подвыборок $ T $ и $ F $, а $ |T|/|Q| $ и $ |F|/|Q| $ — вероятности попадания в соответствующую ветвь.

Этот подход можно применять не только при предсказании (инференсе), но и при обучении дерева.

## Деревья как линейные модели

Решающее дерево можно рассматривать как частный случай линейной модели. Предсказание дерева можно записать как:
$$ y^* = \sum_j c_j [x \in Q_j] $$
где $ Q_j $ — подпространства признакового пространства, соответствующие листьям дерева, а $ c_j $ — предсказанное значение для каждого листа.

Таким образом, в деревьях решений задача линейно разрешима в каждом подпространстве признаков. Построение дерева с использованием жадного алгоритма можно рассматривать как механизм поиска наиболее информативного признакового пространства, в котором задача становится линейно решаемой.

## Алгоритмы построения деревьев

Существует несколько алгоритмов для построения деревьев решений, в том числе:

- **ID3** — один из первых алгоритмов для построения деревьев решений. Основан на использовании энтропии как критерия выбора признаков.
- **C4.5** и **C5.0** — улучшенные версии ID3, которые используют различные усовершенствования, такие как работа с пропущенными значениями и непрерывными признаками.
- **CART (Classification and Regression Trees)** — алгоритм, который поддерживает как задачи классификации, так и регрессии, и использует индекс Джини (или другой критерий) для классификации, а дисперсию для регрессии.

# Bagging (Bootstrap Aggregating)

## Bootstrap

Процедура **Bootstrap** — это метод генерации нескольких выборок из исходной выборки с помощью выборки с возвращением. Если у нас есть исходная выборка $ X $ размера $ m $, то мы многократно (N раз) достаем по $ m $ объектов с возвращением, таким образом получаем $ N $ новых выборок. Один и тот же объект из исходной выборки может попасть в новую выборку несколько раз. 

Пусть для каждой сгенерированной таким образом выборки $ X_j $ мы обучили модель регрессии $ b_j $. Затем для каждой модели можно рассчитать ошибку как:
$$ \epsilon_j(x) = b_j(x) - y(x) $$  
где $ y(x) $ — истинное значение целевой переменной, а $ b_j(x) $ — предсказание модели. 

Матожидание ошибки для каждой модели:
$$ \mathbb{E}_x[(b_j(x)-y(x))^2] = \mathbb{E}_x[\epsilon_j^2(x)] $$

Средняя ошибка по всем моделям:
$$ E_1 = \frac{1}{N} \sum_j \mathbb{E}_x [\epsilon_j^2(x)] $$

Для упрощения анализа делаем два важных предположения:
1. **Несмещённость ошибок**. Предполагаем, что средняя ошибка каждого регрессора равна нулю:
   $$ \mathbb{E}_x[\epsilon_j(x)] = 0 $$
   Это означает, что предсказания моделей в среднем не систематически отклоняются от истинного значения целевой переменной.

2. **Независимость ошибок**. Ошибки между различными регрессорами некоррелированы:
   $$ \mathbb{E}_x[\epsilon_j(x) \epsilon_i(x)] = 0, \text{ при } i \neq j $$
   Это логичное предположение, так как регрессоры обучены на разных бутстрап-выборках, и их ошибки считаются независимыми.

### Построение финальной модели
Для получения финального предсказания усредняем результаты всех моделей:
$$ a(x) = \frac{1}{N} \sum_j b_j(x) $$

Средняя ошибка для финальной модели будет:
$$ E_N = \frac{1}{N} E_1 $$

Таким образом, при соблюдении предположений средняя ошибка финальной модели уменьшается в $ N $ раз по сравнению с ошибкой одной модели.

**Однако**, в реальности гипотезы о несмещённости и независимости ошибок могут не выполняться полностью, особенно предположение о независимости. На практике ошибки различных моделей могут быть коррелированными, что уменьшает эффект снижения ошибки.

## Bias-Variance Tradeoff (Баланс смещения и разброса)

Ошибку предсказания можно разложить на две основные составляющие:

1. **Bias (смещение)** — это систематическое отклонение средних предсказаний модели от истинных значений целевой переменной. Важно отметить, что здесь речь идёт именно о **средних** предсказаниях модели, полученных при обучении **на различных выборках**, а не о предсказаниях на конкретной выборке. Высокий bias означает, что модель плохо описывает зависимость в данных (подобно недообучению).

    Для некоторого объекта $ x $ из данных и его истинного значения $ y(x) $, bias выражается как разница между математическим ожиданием предсказаний модели на множестве выборок $ \mathbb{E}[b(x)] $ и истинным значением $ y(x) $:

$$ \text{Bias}(x) = \mathbb{E}[b(x)] - y(x) $$

Где:
- $ \mathbb{E}[b(x)] $ — среднее предсказание модели $ b(x) $ на всём множестве возможных обучающих выборок,
- $ y(x) $ — истинное значение целевой переменной для объекта $ x $.

2. **Variance (разброс)** — это насколько сильно изменяются предсказания модели при обучении на разных выборках одного генерального распределения. Высокий variance означает, что модель нестабильна и сильно зависит от данных (подобно переобучению).

## Bagging (Bootstrap Aggregating)

**Bagging** — это метод агрегирования нескольких моделей, обученных на разных бутстрап-выборках, для снижения ошибки предсказания. Он особенно эффективен для моделей с низким смещением и высоким разбросом (variance), таких как решающие деревья.

Усреднение нескольких моделей помогает снизить разброс (variance), так как усреднение сглаживает различия между моделями. В результате, хотя каждая отдельная модель может иметь высокий разброс, их комбинация будет более устойчивой.

Если у нас есть $ N $ независимых моделей, то при их агрегировании мы можем уменьшить среднюю ошибку предсказания, поскольку шум в предсказаниях каждой модели, будучи несмещённым, частично компенсируется за счёт случайности. Таким образом, усреднение помогает "отфильтровать" шум, тогда как истинная зависимость сохраняется.

## Влияние зависимости между моделями

Однако важно помнить, что эффективность bagging может снижаться, если модели сильно скоррелированы между собой. Если разные модели делают похожие ошибки, то усреднение не будет так эффективно. В пределе, если все модели совершают одинаковые ошибки, bagging не даст улучшения качества предсказаний.

Кроме того, если у всех независимых моделей отсутствует истинная зависимость в предсказаниях (то есть они все плохо предсказывают таргет), то и усреднённая модель будет давать плохие результаты.

# Random Forest

**Random Forest** (случайный лес) — это метод ансамблевой модели, который использует модифицированный подход бэггинга с решающими деревьями, вводя дополнительную случайность при выборе признаков для каждого разделения. Это позволяет создать менее скоррелированные деревья, что улучшает итоговое предсказание.

Основное отличие от стандартного бэггинга в том, что в каждом узле дерева мы выбираем случайное подмножество признаков, а не используем все доступные признаки, что называется **метод случайных подпространств (Random Subspace Method)**. Это помогает предотвратить ситуацию, когда один или несколько признаков доминируют при разбиении, что может привести к сильной корреляции между деревьями в ансамбле.

### Влияние случайного выбора признаков

Поскольку мы случайным образом выбираем признаки для разбиения в узлах, вероятность того, что оптимальный признак будет выбран, снижается. Однако это уменьшает зависимость деревьев от одного и того же набора признаков, что ведёт к уменьшению корреляции ошибок между деревьями. В результате, при усреднении предсказаний всех деревьев, итоговая модель оказывается более устойчивой и точной, поскольку ошибки отдельных деревьев частично компенсируют друг друга.

### Преимущества

- **Снижение корреляции между деревьями**. Благодаря случайному выбору признаков в каждом узле, деревья становятся менее зависимыми друг от друга. Это увеличивает разнообразие в предсказаниях и снижает корреляцию ошибок, что положительно сказывается на качестве итоговой модели.

- **Устойчивость к переобучению**. За счёт комбинации бутстрапирования и случайного выбора признаков, Random Forest демонстрирует хорошую устойчивость к переобучению, даже если отдельные деревья сильно переобучены.

- **Обработка больших наборов признаков**. Поскольку модель работает с подмножествами признаков, она особенно эффективна при работе с данными, где много признаков, включая неинформативные или избыточные.

## Random Forest и выбросы

Random Forest может справляться с выбросами лучше, чем отдельные решающие деревья, но его устойчивость к выбросам всё ещё ограничена. В задачах регрессии RF использует среднеквадратичное отклонение (MSE) для оценки качества разбиений, а MSE, как известно, чувствительно к выбросам.

# Out of Bag Error (OOB Error)

**Out of Bag Error (OOB)** — это метод оценки качества модели, используемый в ансамблях, таких как бэггинг (Bagging) и случайный лес (Random Forest). Он основан на использовании объектов, которые не попали в бутстрапированную выборку при обучении отдельного дерева, для валидации этого дерева.

## Механизм OOB

При обучении деревьев в рамках бэггинга или случайного леса для каждого дерева создаётся своя бутстрапированная выборка из исходных данных (т. е. случайная выборка с возвращением). В среднем, около **63%** объектов исходной выборки попадают в бутстрап-выборку, а оставшиеся **37%** объектов не используются для обучения конкретного дерева. Эти объекты называются **OOB-объектами** (Out of Bag samples), и они могут быть использованы для оценки модели без необходимости выделять отдельную валидационную выборку.

Таким образом, для каждого объекта из обучающей выборки можно вычислить ошибку предсказания, используя только те деревья, которые не видели этот объект во время своего обучения. Этот метод оценки позволяет получить достоверную оценку качества модели на обучающих данных без необходимости выделять дополнительную часть данных для тестирования.

### Алгоритм расчета OOB ошибки

1. Для каждого дерева создаётся бутстрапированная выборка, а объекты, которые не попали в эту выборку, сохраняются как **OOB-объекты** для этого дерева.
2. После обучения каждого дерева, его OOB-объекты используются для оценки его производительности на новых данных (тех, которые оно не видело при обучении).
3. Для каждого объекта обучающей выборки вычисляется среднее предсказание по всем деревьям, которые не видели этот объект (не использовали его в своей бутстрапированной выборке).
4. Ошибка OOB рассчитывается как средняя ошибка предсказаний на всех объектах обучающей выборки, используя только деревья, которые не включали данный объект в свою бутстрап-выборку.

Формально, **OOB error** — это средняя ошибка предсказания для каждого обучающего примера $ x_i $, рассчитанная на основе только тех деревьев, которые не использовали $ x_i $ в своей бутстрапированной выборке.

Однако следует помнить, что оценка на OOB выборке будет ниже чем оценка на классическом валидационном датасете, так как при оценке на OOB мы не используем полностью весь ансамбль деревьев.

# Детекция аномалий

Метод Isolation Forest применяется для поиска выбросов в данных (детекции аномалий). Аномалия — это объект, не похожий на остальные, который находится в области с низкой плотностью данных. Такие объекты легко отделить от других, поскольку они изолированы.

Основная идея заключается в том, чтобы построить дерево, где на каждом шаге происходит разделение объектов, при этом целью является отделение "непохожих" объектов. Алгоритм использует бутстрапированные выборки и метод случайных подпространств (Random Subspace Method), чтобы создать не одно дерево, а целый лес.

Глубина дерева ограничивается, чтобы не отделять слишком похожие объекты. Для каждого объекта затем вычисляется средняя глубина его отделения в деревьях. Объекты, которые отделялись на малой глубине, скорее всего являются выбросами, так как их легче изолировать.

# Baseline models

Для быстрой разработки моделей на табличных данных можно использовать такие алгоритмы, как **Random Forest**, **kNN** (если выборка не слишком большая и размерность признаков умеренная), **линейная или логистическая регрессия**, **SVM** и **наивный байес** для классификации. Эти модели просты в реализации и позволяют быстро построить базовую модель, которую затем можно улучшать и дорабатывать.

# Сложность деревьев

Сложность работы дерева решений составляет **O(N)**, где **N** — это глубина дерева. 

Также стоит помнить, что дерево — это кусочно-постоянная функция. Это означает, что дерево не способно к экстраполяции за пределы диапазона значений обучающих данных, и за пределами выпуклой оболочки обучающей выборки модель будет предсказывать константные значения.

# Random Forest и категориальные признаки

Random Forest может работать с категориальными признаками. Это реализовано в таких библиотеках, как **CatBoost**, **LightGBM** и **XGBoost**. Например, вместо стандартного бинарного разбиения при каждом узле дерева, можно разбивать на **K поддеревьев** (где K — количество категорий). Это метод называется **бинаризацией**. Также можно использовать классические методы преобразования категориальных признаков в числовые:

1. **One-Hot Encoding** — преобразует каждую категорию в новую бинарную колонку (каждое значение категории кодируется как отдельный бинарный признак).
   
2. **Binary Encoding** — сначала каждая категория преобразуется в целочисленное значение, затем это значение представляется в бинарном виде.
   
    Пример:
    | Color  | Integer | Binary | Encoded1 | Encoded2 |
    |--------|---------|--------|----------|----------|
    | Red    | 1       | 01     | 0        | 1        |
    | Green  | 2       | 10     | 1        | 0        |
    | Blue   | 3       | 11     | 1        | 1        |

    **Binary Encoding** подходит для признаков с высокой кардинальностью, так как уменьшает количество столбцов и более эффективно использует память. Однако есть риск того, что закодированные числа могут случайно передать упорядоченность, которой нет в исходных данных.

3. **Mean Target Encoding** — для каждой категории вычисляется среднее значение целевой переменной (например, вероятность положительного класса для бинарной классификации) и каждая категория заменяется на соответствующее среднее значение.
   
    **Плюсы:** уменьшение размерности данных.  
    **Минусы:** риск утечки данных, если не контролировать влияние целевой переменной на процесс кодирования.

# Особенности работы деревьев с признаками

Одним из преимуществ деревьев решений является их способность работать с признаками независимо от шкалы. Это означает, что нет необходимости в предварительном нормировании признаков. Тем не менее, нормализация может оказаться полезной, так как плотность данных вблизи нуля обычно выше, что может улучшить работу модели на некоторых данных.

# Pasting

Пастинг это анолог бэггинга, только в данном случае мы формируем подвыборки без возвращения элементов в исходную выборку. Это означает, что каждый элемент исходный выборки может быть только в одной подвыборке. By sampling without replacement, it ensures diversity among the training subsets and can help reduce overfitting. However, its effectiveness depends on the size and characteristics of the dataset. Like bagging, it improves model robustness and accuracy by aggregating the predictions of multiple models.

# Отбор признаков с помощью RF

Отбор признаков в случайном лесе (Random Forest) может быть выполнен с помощью различных методов, основанных на важности признаков, вычисляемой по данным обучения. Важность признаков в случайном лесе определяется на основе того, насколько каждый признак улучшает разделение данных на каждом этапе построения дерева.

# Blending

В методе блендинга мы можем использовать несколько различных моделей, например: kNN, логистическую регрессию, случайный лес, наивный байес и дерево решений. Каждая из моделей обучается для задачи классификации и предсказывает вероятности для каждого класса целевой переменной. Поскольку у каждой модели может быть своя интерпретация вероятности, мы можем собрать все предсказанные вероятности от каждой модели и составить их взвешенную сумму. Веса определяются путём обучения на заранее отложенной валидационной выборке. Таким образом, мы получаем новое решение задачи, преобразуя исходные данные в новое пространство признаков, где количество признаков равно количеству моделей (например, 5 признаков — вероятности от каждой модели).