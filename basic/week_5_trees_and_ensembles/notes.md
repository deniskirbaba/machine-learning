# Deiscion Tree
Дерево может решать задачу и регрессии и классификации при условии наличия нелинейных зависимостей между признаками. Однако DT склонен к переобучению (при большой глубине).

На каждом шаге дерева происходит разбиение выборки на две подвыборке на основе сравнения элементов выборки с некоторым пороговым значением определенного признака. Это происходит рекурсивно, до тех пор пока не будет достигнута максимальная глубина дерева.

Как разбивать на подвыборки?
На каждом шаге решается оптимизационная задача: $|T|/|Q| H(T) + |F|/|Q| H(F) -> min_{j, t}$, где Q - изначальная выборка, T и F - выборки после разделения $x_j < t$, H - некоторый функционал неоднородности множества.

Как задать H?
H - мера неупорядоченности (неоднородности) (heterogenity) данных.
Будем рассматривать задачу бинарной классификации.
1. Энтропия: H(T) = -p_0 log(p_0) - p_1 log(p_1)
В логистической функции потерь было что-то похожее (-p log(q) - q log(p)) - это кросс-энтропия (в бинарном случае) - то есть перекрестная энтропия между двумя распределениями (истинное распределение и распределение предсказанное). А в случае деревьев у нас одно распределение и мы пытаемся посчитать логарифм правдоподобия по нему.
2. Gini impurity: H(T) = 1 - p_0^2 - p_1^2

Эти меры обобщаются на многоклассовый случай тривиально.

Решающие деревья адаптивны к задачам регрессии и классификации (бинарной и многоклассовой), так как потребуется только лишь поменять критерий информативности H, и мы сможем решать эту задачу (там просто "жадный" алгоритм перебора всех признаков по всем порогам). 

Gini impurity количественно описывает вероятность вытащить два объекта из разных классов (в случае бинарной классификации).

Для вычисления оптимального порога для фичи мы "жадным" образом перебираем все пороги и все фичи.

Для вычисления предсказания производится проход по дереву и в окончательном листе вычисляется вероятность принадлежности к тому или иному классу (через частоты объектов каждого класса). 

Для использовании дерева решений в задаче регрессии надо поменять критерий информативности. Если в случае классификации мы в каждом "кусочке" пространства признаков предсказывали константую вероятность, то в задаче регрессии будем предсказывать константу таргета.
Например, вид меры информативности для разбиения в задаче регрессии: $H(T) = min_c 1/|T| \Sum_k (y^{(k)} - c)^2$. Просто минимизируем среднее квадратичное отклонение от среднего в каждом листе.

Pruning - усечение деревьев.
1. Pre-Pruning используется глубоко-глубоко под капотом в алгоритме построения дерева или не используется вообще
2. Post-Pruning. После того как мы уже построили дерево мы можем задать какой-то "жадный" алгоритм, который будет объединять ветви деревьев на основе какого-то критерия. 

Нейронные сети тоже любят усекать определенным образом. 

Так как дерево - это кусочно-постоянная функция, то оптимизировать его градиентным методом не имеет смысла.
Множество параметров дерева (признаки и пороговые значения) наращиваются с увеличением глубины дерева - а значит будет переобучение.

Гиперпараметры дерева: критерий останова (можно строить дерево до исчерпания энтропии (но будет сильно переобученное дерево), логичнее останавливать разбиение дерева когда прирост информативности не станет меньше определенной константы, либо сделать ограничение на глубины выборки, либо ограничить минимальный размер листа).

Деревья поодиночке почти нигде не используются, они используются в виде ансамблей. 

Дерево устойчиво к пропускам в данных. При встрече объекта с пропущенным значением мы идем по обоим веткам данного узла и вычисляем значение таргета. Далее, используя вероятность для выбора каждого поддерева (вычисленная как частота классов при обучении), оцениваем таргет как: $y^* = |T|/|Q| * y^*_T + |F|/|Q| * y^*_F$. 
При инференсе это легко реализовать, однако и при обучении мы можем это реализовать тоже. 

Дерево можно рассматривать как линейную модель, то есть: $y^* = \Sum_j c_j [x \in Q]$. Где Q подпространство признакового пространства, соответствующее листу дерева. То есть теперь у нас есть взвешенная сумма новых признаков $[x \in Q]$.
Построение дерева с помощью "жадного" алгоритма можно рассматривать как некоторый механизим поиска более информативного признакового пространства в котором задача линейно разрешима.

Алгоритмов построения деревьев много: ID-3, C4.5, C5.0, CART...

# Bootstrap
Процедура Bootstrap это процедура генерации множества выборок с помощью доставания определенного числа объектов из исходной выборки с возвращением. То есть пусть есть выборка X размера m, мы достаем m объектов с возвращением и делаем это N раз, таким образом получаем N выборок. То есть один объект из исходной выборки может много раз попасть в новую выборку. 
Пусть для каждой сгенерированной таким образом выборки X_j мы построили модель регрессии b_j. Затем посчитаем среднее квадратичное отклонение для каждой модели по ошибке $\epsilon_j(x) = b_j(x) - y(x)$. То есть мат ожидание ошибки для каждого регрессора: $\mathbb{E}_x[b_j(x)-y(x)]^2 = \mathbb{E}_x[\epsilon_j^2(x)]$.
Тогда можем посчитать среднюю ошибку по каждой модели: $E_1 = 1/N \Sum_j \mathbb{E}_x \epsilon_j^2(x)$.
Делаем два предположения:
1. Мат ожидание ошибки для каждого регрессора в среднем не завышено и не занижено, то есть равно 0: $\mathbb{E}_x[\epsilon_j(x)] = 0$
2. Ошибки для двух различных регрессоров не скоррелированы: $\mathbb{E}_x[\epsilon_j(x) \epsilon_i(x)] = 0, if i != j$. Логичное предположение, так как мы обучали регрессоры на разных (хоть и бутстрапированных) выборках

Построим финальную модель: $a(x) = 1/N \Sum_j b_j(x)$ - то есть усредняем предсказания от всех регрессоров. 
При расчете средней ошибки финальной модели получаем: $E_N = 1/N E_1$. То есть ошибка упала в N раз. 
ОДНАКО, гипотеза о том что ошибки у нас нескоррелированы и несмещенные (1 и 2), вообще говоря, ложь (особенно про нескореллированность).

# Bias-variance
У нас у ошибки предсказания есть bias (смещение) и variance (разброс). Смещение - насколько предсказание модели в среднем отличается от истинного значения таргета. Разброс - неустойчивость модели (насколько одна и та же модель будет давать разные результате на выборках из одного и того же генерального распределения) (то есть малое изменение обучающей выборки приводит к значительному изменению предсказаний модели). 

# Bagging = Bootstrap AGGregatING
Bagging позволяет кучу моделей которые между собой несогласованы, использовать чтобы понизить ошибку. Потому что у них у всех bias будет маленький, а variance большой, поэтому их усреднение (нескольких моделей) позволит нам понизить variance.

То есть если у нас есть N независимых алгоритмов предсказания, то аггрегировав их мы тем самым уменьшаем в среднем ошибку предсказания (так как шум в предсказании каждого алгоритма несмещенный). Таким образом, мы уменьшили шум в итоговом предсказании (так как шум в отличии от истинной зависимости может быть отфильтрован из-за своей случайности), а истинная зависимость не поменялась (которая была у каждого алгоритма).

Однако может такое случиться, что у независимых алгоритмов нет истинной зависимости в предсказаниях, тогда и у финальной модели предсказания будут плохие.

# Random Forest
Случайный лес отличается от Bagging над деревьями тем, что мы случайным образом выбираем признаки для разделения выборки в узлах. Это называется Random Subspace Method - метод случайных подпространств. 
То есть мы выбираем случайные признаковые подмножества из которых мы будем выбирать лучший признак. Так как мы выбираем случайные признаки, то скорее всего оптимальный по выборке признак мы НЕ выбираем. А значит деревья будут учиться не только на бутстрапированных выборках, а еще и на случайных подмножествах признаков (при чем в каждой вершине). Таким образом ошибки будут еще менее скоррелированы, а значит при аггрегации всех деревьев оценка целевой переменной будет лучше. 

RF может справляться с выбросами (конечно не особо, так как при регрессии внутри есть СКО, а при выбросах это не особо хорошо) и с пропусками в данных (так как RF основан на деревьях).

# Out of Bag Error

Есть свойство, которое есть и у bagging и у RF: OOB - out of bag estimation (не попавший в бутстрап выборку). Для дерева, обучающегося на определенной бутстрапированной выборке, объекты, которые не попали в эту бутстрапированную выборку можно использовать как валидационные объекты для этого дерева. То есть запоминая, какие объекты на каких деревьях не попали в бутстрапированную выборку, мы может потом провалидироваться на этих деревьях на этих определенных объектах.
OOB error is the mean prediction error on each training sample $x_i$, using only the trees that did not have $x_i$ in their bootstrap sample.

# Детекция аномалий

Есть также Isolation Forest, которые могут применяться для поиска выбросов в данных (детекция аномалий). По своей сути, аномалия - это объект не похожий на остальные объекты, а значит вокруг него есть какая-то зона разряжения. Соответственно так как объект находится далеко от всех остальных, то его можно легко отделить. Это заключение можно положить в алгоритм построения дерева: будем просить чтобы на каждом шаге дерево пыталось отделить один непохожий на остальные объект. ПРи этом мы будем это делтаь на бутстрапированных выборках и исопльзуя RSM, тем самым построив не одно дерево, а целый лес. При чем можем ограничить глубину дерева, чтобы не отделять похожие объекты. Затем для каждого объекта мы можем посчитать на какой в среднем глубине он был отделен (для каждого дерева). Соответственно мы получили отранжированные объекты по глубине - те объекты которые отделялись на малой глубине - вероятно выбросы. 

# Baseline модели
Random Forest, kNN (если выборка не слишком большая и не слишком многомерная), линейная или логистическая регрессия и наивный байес для классификации.
Эти модели можно использовать если нам требуется быстро построить модель и дальше её дорабатывать.
НА ТАБЛИЧНЫХ ДАННЫХ

Сложность деревьев O(N), где N - глубина дерева. 

Также стоит помнить, что дерево - кусочно-постоянная функция, а значит дерево не способно к экстраполяции кроме константных значений.

Бутстрапирование вообще говоря полезная вещь - она применяется также для получение более точных оценок различных статистик по выборке.

Также RF можно применять для категориальных фичей, это реализовано в CatBoost, LightGDM, XGBoost. Например можно разбивать не на 2 поддерева при каждом разбиении, а на K поддеревьев (где K-число категорий). Или же использовать стандартные подходы для преобразования категориальных фичей в числовые форматы:
1. OneHot encoding - transforms each category in a new binary column
2. Binary encoding - first convert category into integer and then represent these integers as binary numbers. 

Color	Integer	Binary	Encoded1	Encoded2
Red	    1	    01	    0	        1
Green	2	    10	    1	        0
Blue	3	    11	    1	        1

Binary encoding is suitable for high cardinality features. More memory efficient with reduced dimensionality. But there is still some risk of the encoded integers implying ordinality!
3. Mean target encoding. For each category within a categorical feature, compute the mean of the target variable (e.g., if the target variable is binary, this would be the probability of the positive class). Replace each category in the feature with its corresponding mean target value.
Pros: Dimensionality Reduction
Cons: RISK OF LEAKAGE!!!

Также ансамбли деревьев можно хорошо распараллеливать для увеличения скорость их инференса на GPU.

Также плюс деревьев и их ансамблей в том, что он работает с фичами объекта независимо, а значит не важно в каких шкалах каждый признак. НО нормировать данные всё равно полезно, даже по той причине что плотность точек около нуля выше.

# Pasting
Пастинг это анолог бэггинга, только в данном случае мы формируем подвыборки без возвращения элементов в исходную выборку. Это означает, что каждый элемент исходный выборки может быть только в одной подвыборке. By sampling without replacement, it ensures diversity among the training subsets and can help reduce overfitting. However, its effectiveness depends on the size and characteristics of the dataset. Like bagging, it improves model robustness and accuracy by aggregating the predictions of multiple models.

# Отбор признаков с помощью RF
Отбор признаков в случайном лесе (Random Forest) может быть выполнен с помощью различных методов, основанных на важности признаков, вычисляемой по данным обучения. Важность признаков в случайном лесе определяется на основе того, насколько каждый признак улучшает разделение данных на каждом этапе построения дерева. Рассмотрим основные методы и шаги по реализации отбора признаков в случайном лесе.