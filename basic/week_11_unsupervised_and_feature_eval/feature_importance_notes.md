# Оценка значимости признаков

From this video: https://www.youtube.com/watch?v=pkqQSygcFNk&list=WL&index=6

## Bias-Variance(-Noise) decomposition

Ошибка раскладывается как шум, смещение и разброс. 

В слайдах ошибка раскладывается на компоненты в задаче линейной регрессии с MSE функцией потерь.

Шум: E_{x,y}[(y - E[y|x])^2]. То есть если бы у нас был детерминирован y (мы могли бы однозначно определить его для какого-то x), то у нас шум был бы нулевой, однако если это не выполняется, то шум будет ненулевой.

Разброс: E_{x}[E_{X}[(mu(X) - E_{X}[mu(X)])^2]]. X - выборка, mu(X) - предсказания модели на выборке. Мы семплируем выборку X из всего распределения x. В последних скобках - дисперсия предсказания модели на выборке. То есть разброс по сути показывает, насколько у нас предсказания модели будут изменяться при изменении выборки.

Смещение: E_{x}[(E_{X}[mu(X)] - E[x|y])^2] - насколько точно мы предсказываем таргет нашей моделью в среднем по распределению по выборке.

Переобучение - запоминание моделью зависимостей, свойственных ТОЛЬКО обучающей выборке.

## Оценка значимости признаков

Если модель сложная и использует некоторую внутреннюю структуру данных, то использовать простые методы для расчета значимостей признаков - очень плохая идея.

### Линейные модели
В линейных моделях мы можем оценить значимость признака посмотрев на вектор (матрицу) весов. Чем больше вес, чем больше значимость.

Весовые коэффициенты явно указывают какой линейный вклад внесли соответствующие признаки в значение таргета.

Однако это работает только в случае обучении модели на отнормированных данных.

Можно еще рассчитать стандартизованные коэффициенты значимости (в этом случае можно не нормировать выборку):
w_i * std(x_i)/std(y)

При добавлении регуляризациив линейную модель это позволяет добиться определенных свойств:
1. L2 (ridge) - все веса стараются быть одинаково маленькими (около нуля, но не ноль). Например, если у нас будет некоторая мульти-коррелирующая группа признаков, то rigde регуляризация веса этих признаков равномерно уменьшит до маленьких значений. 
2. L1 (lasso) - отбор всех наиболее значимых признаков, а все остальные будут занулены. 

Поэтому есть следующее правило:
* если у нас есть множество признаков, которые могут влиять на таргет не очень сильным образом, то можно использовать L2
* если же мы предполагаем что у выборки есть лишь несколько наиболее значимых признаков, а остальные не важны, то мы можем использовать L1 

Регуляризация = метод учета некоторых априорных знаний при решении задачи.

### Desicion trees

Значимость признаков при использовании деревьев можно оценить как среднее уменьшение энтропии (или в целом изменения любого выбранного критерия) при использовании признака при всех решаюших правилах деревьев.

Можно считать эту величину по-разному:
* по числу разбиений, в которых использовался этот признак
* по среднему уменьшению критерия
* взвешенно по числу разделенных объектов

ОДНАКО - это не самый хороший способ считать значимость, так как в целом мы могли бы обойтись и без самых важных (по рассчитанной таким образом значимости) признаков. 

Так же это всё можно рассчитать не по одному дереву, а по целым ансамблям. Рассчитывая эти величины в среднем по ансамблю.

ОДНАКО такие методы вычисления значимости признаков в деревьях дают смещенную оценку значимости признаков - оценка смещена в сторону непрерывных признаков, а так же дискретных признаков с большим возможным числом значений.

Большим плюсом деревьев и линейных моделей является интерпретируемость модели - мы по объекту можем явно сказать каким образом мы выбирали его таргет (пройти весь путь решающих правил). 

Интерпретируемость моделей важна в определенных областях (например область правового регулирования - пример модель которая выносит уголовный приговор - хотелось бы понимать почему именно был выбран тот или иной приговор)

Использовать одну модель для оценки значимости признаков и другую модель для предсказания таргетов нельзя. Так как всегда вычисленные значимости признаков определяются только для конкретной модели! То есть такие вычисленные значимости оценивают значимость признаков конкретно для модели, а не для самой зависимости выборки. 

Однако специальные методы LIME, SHAP позволяют оценивать значимость признаков, строя некоторые локальные модели (которые в некоторой локальной окрестности хорошо согласовываются с нашей целевой моделью) и на основе этих локальных моделей определяют значимость.

Если какие-то признаки являются неинформативными на основе вычисленных значимостей для одной модели, то это не будет означать что эти признаки будут также неинформативными и для другой модели.

Можно обучить несколько разных моделей и посчитать для каждой из них значимости признаков и усреднить их все - так мы получим чуть более понятную картину.

Анализируя значимость признаков мы можем построить более простое решение задачи.

### Model-agnostic метод значимости признаков

#### Drop feature importance
Можем просто обучать модель с каким-то выкинутым признаком. И сделаем так для каждого признака и оценим работоспособность таких моделей.
На основе этого можем сказать, какие признаки важны. 

Однако минус в том, что для такого способа нам нужно обучать большое число моделей.
Также минус в том, что если например у нас есть продублированные признаки, то будет плохо. И Если например у нас есть группа признаков, которые в совокупности дают информативный признак, а мы выкинули один из признаков этой группы, то и вся группа потеряла информативность.

### Permutation importance (работает уже на обученной модели)

Если у нас модель про которую мы ничего не знаем. Мы только можем получать от нее выход, передавая вход. Черная коробка.

Permutation importance - shuffle feature values within the dataset and evaluate the trained model.

То есть мы перемешиваем значения внутри одной фичи в тестируемой выборке и затем оцениваем предсказания модели - и смотрим насколько у нее упало качество прогноза.
То есть мы видим, насколько шум (потому что мы взяли случайные значения, но из тех же шкал что и реальные значения) в одном конкретном признаке (при том что все остальные остались теми же как и в данных) влияет на качество прогнозов.
Если качество прогнозов уменьшилось, то этот признак важен, если особо не изменилось, то занчит этот признак не особо важен.

### LIME - Local Interpretable Model-agnostic Explanations

1. Пусть у нас есть некоторая уже обученная модель
2. Вводим некоторое семейство "простых" моделей, которое будет в некоторой локальной окрестности пространства входов аппроксимировать нашу уже обученную модель. Также вводим некоторую функцию близости объектов в окрестности (например, экспоненциальная близость) и функцию сложности "простых" моделей.
3. Тогда наша цель (цель метода LIME) это найти такие параметры простых моделей для каждой окрестности пространства, чтобы уменьшить меру аппроксимации простой моделью обученной модели

То есть мы берем некоторую окрестность и аппроксимируем её например линейной моделью с регуляризацией. Затем найдя параметры весов, можем посмотреть какие признаки наиболее значимы именно в этой окрестности. 

На смену LIME пришел SHAP

### Shapley values. SHAP - SHapely Additive exPlanations

https://github.com/shap/shap
https://shap.readthedocs.io/en/latest/

Предлагается взять 2 модели и обучить одну на некотором подмножестве множества признаков, а другую обучить на это же подмножестве, но с некоторой выкинутой фичей. Затем повторить это для некоторого числа подмножеств и усреднить некоторым образом (там будет коэффициент который показывает скольким образом мы можем выбрать подмножество некоторого размера из всего множества признаков) различия предсказаний этих моделей. Полученное значение (shapley value) будет являться значимостью выкинутого признака.

(подробные формулы в pdf файле feature_importance)

Получается, что чтобы посмотреть на значимость признаков нам требуется обучить огромное число моделей. Но зато эти Shapley value говорят по сути о значимости признака (с точки зрения теории игр).

По сути у нас в SHAP тоже будет какая-то локально аппроксимирующая модель.

Также в работе про SHAP предлагается 3 свойства:
1. Локальная точность: в некоторой области у нас предсказание модели может быть представлено в виде какой-то контанты + взвешенная сумма признаков, где в качестве весов являются shap values (вычисленные значимости)
2. Если признак отсутствует, то его значимость должна быть гарантированно = 0
3. consistency: формула в pdf

В представленной работе по SHAP доказывается теорема, что это единственный способ оценить значимость признаков для модели, который будет удовлетворять свойствам выше.

На практике оценка значимости признаков с помощью SHAP предполагает следующее:
пусть у нас в каждой точке выборки есть какое-то предсказание целевой переменной и мы будем смотреть на 2 вещи:
1. ожидаемое значение (везде это будет мат ожидание, однако при разных подномжетсвах мноежства признаков) целевой переменной (например среднее по всей выборке) - то есть когда у нас вообще нет никаких признаков (например для дерева в этом случае у нас будет случай что все признаки пропущены и мы идем по всем веткам и затем усредняем значения в листах с коэффициентами мощностей попавших туда выборок)
2. как наличие каждого признака (то есть мы последовательно добавляем по однмоу признаку и от него считаем предсказания модели) сдвигает наше предсказание от среднего (предыдущего среднего по меньшему подмножеству признаков) к тому предсказанию которое мы получили

Это понятно нарисовано на картинке shap_explain.png

То есть значение вектора shap показывает насколько измениться предсказание нашей модели от математического ожидания предсказаний если она будет учитывать соответствующий этому значению shap признак.

SHAP прекрасно работает с деревьями, так как по ним можно легко вычислять мат ожидание ответа модели для выборки с некоторым подмножеством признаков, НЕ СТРОЯ ДОПОЛНИТЕЛЬНЫЕ МОДЕЛИ И НЕ ОБУЧАЯ ИХ НА РАЗНЫХ ПОДВЫБОРКАХ ПРИЗНАКОВ. То есть можно исопльзовать одну модель, построенную для всех признаков для расчета всего вектора shapley values.

SHAP - лучший способ оценки значимости признаков классических моделей (не нейронок).

То есть для ансамблей деревьев интерпретировать ответ для какого-то входа достаточно сложно, а вот оценить значимость признаков можно достаточно хорошо используя SHAP.

Графики SHAP (как график самого вектора SHAP, так и график значений SHAP по всей выборке) очень информативны и позволяют проводить хороший анализ нашей выборки.

Для нейронок SHAP тоже используется, однако там уже вычисляется некоторая аппроксимация вектора shapley values. И он оценивается некоторыми градиентными методами. 
Например DeepExplainer и GradientExplainer 

### (Grad-)CAM(++)

Рассмотрим методы оценки значимостей признаков для данных со сложной внутренней структурой.

#### CAM - Class Activation Mapping
Делаем усредняющий (по длине-ширине) слой после сверточных слоев. И затем Этот вектор (длиной с depth) используем для предсказания (через полносвязный слой (линейнуюю регр/класс)). И затем мы прокидываем обратно эти веса на первый слой и соответственно можем получить картинки для каждого класса с пикселями, которые наиболее всего вносят вклад в предсказание.

Это метод для определенной архитектуры нейронной сети.

### GradCAM
Тут мы просто дифференцируем наши предсказания по входным пикселям. И таким образом получаем Class activation maps.

### GradCAM++
Усовершенствование GradCAM, там исопльзуется relu под капотом для отбрасывания всех отрицательных импактов и еще чето.