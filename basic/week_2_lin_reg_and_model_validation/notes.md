Главное отличие параметров от гиперпараметров - зависимость от данных.
Параметры модели зависят от данных, в то время как гиперпараметры - нет.

При разбиении датасета на две составляющие: train и test.
При подборе гиперпараметров на train выборке и оценке их эффективности на test выборке будет происходить "утечка данных". Она происходит из-за того что мы, как эксперт будем являться "методом оптимизации" и просто-напросто подбираем лучшие гиперпараметры под тестовую выборку. Из-за этого будет наблюдаться переобучение на гиперпараметрах.
Чтобы избежать этого самым простым решением будет выделение еще одной части из датасета.
То есть теперть будем разбивать: train, validation, test.

Регуляризация в целом - это такие "искусственные" ограничения на нашу модель, которые стабилизируют решение.

Градиентный спуск - итеративный метод поиска минимума функции потерь. Имеет меньшую вычислительную сложность, нежели аналитическое решение. Стохастический градиентный спуск использует не всю выборку при расчете градиента, а только некоторое подмножество. Таким образом, для поиска минимума обычному градиентному спуску потребуется меньше шагов, чем стохастическому. Однако из-за того, что скорость вычисления шага значительно меньше у SGD, то общее время поиска минимума меньше.