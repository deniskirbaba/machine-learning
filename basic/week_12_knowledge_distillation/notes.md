# Knowledge Distillation

[Source video](https://www.youtube.com/live/9m4ytptOer4)

## Общие принципы

Knowledge distillation (дистилляция знаний) — это процесс передачи знаний от сложной, хорошо обученной модели (teacher) к более простой модели (student). Цель дистилляции — сделать student модель легче и быстрее, сохраняя при этом её высокую производительность, близкую к teacher модели.

## Интуиция

При обучении модели для задачи многоклассовой классификации она не только предсказывает истинный класс, но и вычисляет распределение вероятностей принадлежности объекта к разным классам. Это распределение несёт в себе важную информацию о том, на какие другие классы объект похож, а на какие — нет. Эти дополнительные данные могут помочь обучить другую модель.

## Механизм дистилляции знаний

В процессе дистилляции сначала обучают сложную модель teacher с нуля, используя стандартные подходы (объект-класс). Модель teacher изучает сложные зависимости и хорошо обобщается на новых данных. Затем эту модель используют для обучения более простой модели student. Для этого вместо истинных меток классов подают вероятности классов (предсказания teacher модели). Таким образом, student модель учится имитировать поведение teacher модели, хотя изначально она не была бы способна обучиться на тех же данных с такой же точностью.

## Ключевые идеи

Teacher модель преобразует данные в более информативное представление, которое затем используется для обучения модели student. Вместо простой кросс-энтропии по меткам классов используется кросс-энтропия между распределениями вероятностей teacher и student моделей.

Для этого применяют **softmax с параметром температуры**.

### Параметр температуры

Использование температуры $ T $ в функции softmax управляет тем, насколько "плавно" распределяются вероятности. При более высокой температуре вероятности становятся более равномерными, что позволяет student модели лучше учиться на промежуточных представлениях teacher модели.

## Производная функции потерь

При обучении student модели, функция потерь $ C $ — это кросс-энтропия между распределениями вероятностей teacher $ p_i $ и student $ q_i $:

$$ \frac{dC}{dz_i} = \frac{1}{T}(q_i - p_i), $$

где $ z_i $ — выходные значения student модели, $ p_i $ — вероятности teacher модели, а $ q_i $ — вероятности student модели.

При большом значении температуры $ T $, производная функции потерь будет приближаться к следующему выражению:

$$ \frac{dC}{dz_i} = \frac{1}{NT^2}(z_i - v_i), $$

что является производной среднеквадратичной ошибки (MSE).

## Переход к регрессии

Таким образом, при высокой температуре задача классификации становится аналогичной задаче регрессии между логитами teacher и student моделей, с использованием MSE. Это важно, так как позволяет обучать student модель не только по финальным предсказаниям, но и по промежуточным представлениям.

### Глубинное обучение

Мы можем пойти дальше и обучать student модель повторять не только конечные предсказания teacher модели, но и её промежуточные слои. Например, если у teacher модели больше слоёв, чем у student, мы можем заставить student модель имитировать каждую вторую активацию teacher модели. Это даёт student модели более информативные сигналы для обучения и улучшает её способность к обобщению.

## Применение температуры

Параметр температуры $ T $ контролирует, насколько сильно student модель фокусируется на предсказаниях teacher модели. При высоких значениях температуры предсказания становятся более плавными, что помогает student модели лучше улавливать общие закономерности. Таким образом, мы можем управлять степенью "внимания" student модели к различным аспектам поведения teacher модели.

## Преимущества дистилляции

Дистилляция знаний позволяет сначала обучить сложную и мощную модель teacher для решения задачи, а затем "дистиллировать" её знания в меньшую и более быструю модель student. Это даёт значительное преимущество: меньшая модель работает быстрее и требует меньше вычислительных ресурсов, при этом сохраняя почти такое же качество, как и teacher модель.

Важно понимать, что дистилляция — это не способ обучения модели с нуля, а метод передачи знаний от уже обученной модели для создания более лёгкого аналога.

## Пример с несколькими teacher моделями

В одном из подходов к knowledge distillation предлагается обучить несколько teacher моделей на различных доменах (например, на разных типах изображений), а затем дистиллировать их знания в одну итоговую student модель. В результате student модель может достичь более высокого качества, чем любая из отдельных teacher моделей, поскольку каждая teacher модель специализируется на своём домене и передаёт ключевые знания в общую модель.

# Usage of Pretrained Models and Transfer Learning

[CS231n: Transfer Learning](https://cs231n.github.io/transfer-learning/)

## Основные понятия

- **Обучение (training, fitting)** — это процесс настройки параметров модели для решения конкретной задачи, основанный на обучающей выборке.
  
- **Transfer Learning (перенос обучения)** — это метод, который позволяет использовать знания, полученные моделью на одной задаче или домене, для решения другой задачи или работы в другом домене. Например, если у нас есть модель, обученная на большом корпусе текстов (например, Википедии), и мы хотим адаптировать её для обработки медицинских текстов, мы можем использовать transfer learning.

## Варианты Transfer Learning

Существует два основных подхода к переносу обучения: **feature extraction** и **fine-tuning**.

### Feature Extraction (извлечение признаков)

В этом подходе мы используем предобученную модель как инструмент для извлечения признаков (feature extractor). Основная идея заключается в том, что слои модели, обученные на большой и разнообразной задаче, могут эффективно извлекать полезные признаки, даже если сама задача изменилась.

1. **Замораживаем (freeze)** все слои модели, кроме последнего. Это значит, что параметры этих слоев не будут изменяться в процессе дальнейшего обучения. Модель будет использоваться для генерации эмбеддингов (признаков) на основе входных данных.
2. Добавляем новый последний слой (или несколько слоев), которые соответствуют новой задаче.
3. Дообучаем только новый слой(и) на новых данных для адаптации к текущей задаче.

Этот подход полезен, когда данные для новой задачи ограничены, так как большая часть параметров модели остаётся неизменной.

### Fine-Tuning (тонкая настройка)

В этом случае модель полностью дообучается на новой задаче. Здесь предобученная модель используется как начальная точка (инициализация) для дальнейшего обучения.

1. **Размораживаем все слои модели** и продолжаем обучать её на новых данных.
2. Параметры модели адаптируются под новую задачу, что позволяет улучшить её производительность на специфической задаче или домене.

Этот подход требует больше данных и вычислительных ресурсов, но даёт более высокую гибкость, так как модель подстраивается под новую задачу более точно.

## Связь с Knowledge Distillation

Дистилляция знаний может быть связана как с transfer learning, так и с fine-tuning:

- Мы можем использовать дистилляцию для создания более простой модели (student) из сложной модели (teacher), что помогает в решении задачи на новом домене.
- Fine-tuning может включать элементы дистилляции, когда обучаемая модель перенимает знания от нескольких teacher моделей, специализирующихся на различных частях задачи или доменах.
