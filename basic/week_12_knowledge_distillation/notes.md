# Knowledge distillation

https://www.youtube.com/live/9m4ytptOer4

Архитектура для различных целей тоже различная

Модель в процессе обучения для задачи многоклассовой классификации учится не только предсказывать истинный класс, но и учится некоторому обобщению. То есть она выдаёт нам вероятности принадлежности объекта к классу, а не только один класс (как мы подаем объект на обучение). А значит исходя из этой информации (вероятности принадлежности к классам) можно выделить полезную инфу. Например будет видно, на объекты каких классов данный объект похож и на какие классы он вообще не похож и т.д. 

То есть можно использовать эту информацию.

Дистилляция знаний - это когда мы обучили с нуля (просто подавая объект - класс) некоторую сложную модель, которая выучила зависимости и достаточно хорошо генерализируется на новые данные, а затем, используя пары [объект-вероятности классов объекта (выход с уже обученной модели)] мы обучаем простую модель, которая не смогла бы с нуля выучить такие зависимости. То есть вторая (простая) модель по сути учится просто повторять поведение первой, так как она с нуля не смогла бы обучиться (так как недостаточно сложная).

Эти модели называют teacher-student.

То етсь модель teacher делает данные более информативными и подаёт их для обучения модели student.

При обучении student модели будем использовать кросс-энтропию между распределениями модели teacher и student.

Софт-макс будем использовать с параметром температуры.

Тогда вычисляя производную по функции потерь от выходов модели студента:
dC/dz_i = 1/T (q_i - p_i), где C-кросс энтропия между распределениями teacher и studetn, z - вектор выхода модели student, v - вектор выхода модели teacher, p - вектор вероятностей модели teacher, q - веткор вероятности модели student.
Вообще говоря, большом значении температуры T у нас эта производная будет равна:
dC/dz_i = 1/(NT^2) (z_i - v_i) - а это является производной MSE.

То есть внезапно у нас получается, что задача классификации становится эквивалентной задаче регрессии логитов учителя со среднеквадратичной ошибкой.

То есть у нас производная пропорциональная линейному отклонению при высокой температуре. 

То есть мы можем "учить" модель student повторять любые промежуточные представления модели teacher, так как у нас это выродилось в задачу MSE между логитами этих моделей, а значит можем и пойти вглубь в другие слои (представления).

То есть мы можем показывать модели student как перейти не сразу из входных данных в целевую переменную, а можем показать и как переходить последовательно из входных данных в промежуточные представления и затем уже в целевую переменную.

То есть например, будем учить повторять каждые вторые промежуточные слои модель teacher моделью student (у student будет в 2 раза меньше слоев). Тем самым мы показываем модели какие промежуточные слои она должна получить, а не только показывая ей финальный ответ. Тем самым мы даем ей более информативный сигнал для обучения. 

Также мы можем легко перейти от задачи классификации к задаче регрессии в задаче Knoledge distillation.

Стоить помнить, что мы делали предположение о высокой температуре, это значит что с помощью значения температуры мы можем контролировать то, насколько сильное внимание обращает модель student на предсказания модели teacher.

Данный подход позволяет вначале обучать огромную и сложную модель для того чтобы научиться решать задачу, а потом дистиллировать её в гораздо более маленькие модели. И получается эти маленькие модели будут работать гораздо быстрее и практически без потерь качества.

То есть дистилляция это не способ обучения модели с нуля, а способ эксплуатации уже обученной модели для создание более легковесного её аналога.

То есть можем использовать уже обученные модели для того, чтобы дистиллировать из них знания для новой собственной модели.

Например, в статье по Knoledge distillation было предложено обучить несколько моделей teacher на различных доменах (например на различных типах картинок) и затем их все дистиллировать в одну итоговую student модель. И в итоге эта итоговая модель показывала на всем датасете качество лучше, чем любая другая модель, которая была обучена на датасете сходу. ТО есть с помощью "разведки" моделями teacher (посколько эти большие модели teacher досканально изучают каждая свой домен, а затем уже передают только важную инфу модели student), мы смогли построить модель student не только которая быстрее работает, но и которая дает более лучшие результаты.

По сути, дистилляция это обучение используя не только выборку, а еще и готовую модель, которая достаточно хорошо умеет давать предсказания.  

# Термины

Обучение - training, fitting

Дообучение (берем уже обученную модель и дообучаем ей под какуюто задачу, под последние данные и тд ) - fine-tuning (тонкая настройка)

Transfer learning - учимся перетаскивать знания из одной области в другую (допустим у нас есть модель которая обучена работать с текстом целиком, например обученная на википедии, а теперь нам надо перенести эту модель на домен медицинских тектов)

А дистилляция может относиться как к transfer learning (можем дистиллировать общую модель из частных teacher моделей), так и к fine-tuning