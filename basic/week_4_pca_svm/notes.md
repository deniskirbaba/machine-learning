## SVM

Отступ (margin) - это по сути проекция вектора (объекта из выборки) на разделяющую гиперплоскость.
В SVM мы максимизируем зазор между двумя классами точек. На объекты которые находятся на расстоянии меньшем 1 от разделяющуй плоскости мы вводим штрафы. 

Изначальная постановка задачи такая: пусть у нас выборка линейно-разделимая и тогда мы максимизируем ширину разделяющей полосы, а значит мы минимизируем вторую норму вектора весов. Затем мы задали ограничение на margin для каждого объекта.

Соответственно все объекты в задаче SVM делятся на 3 типа:
1. Неинформативные объекты, которые лежат вглубине класса (по правильную сторону). У них $M > 1$. Они не вносят импакта в $w$
2. Опорные объекты, лежат непосредственно на разделяющей полосе. $M = 1$
3. Нарушители: $M < 1$

В итоге получается функция эмпирического риска в которой вид функций регуляризации и потерь поменяны местами с задачей линейной регрессии с регуляризацией. То есть вообще говоря оптимизационная задача может иметь несколько членов, который нам нужно минимизировать и мы составляем их линейную комбинацию, а как назвать каждый отдельный член не важно. По факту все эти члены составляют функцию эмпирического риска и важно понимать что откуда пришло, а не как что назвать.

Коэффициент $C$ имеет следующую зависимость:
1. Если он большой, то это означает что у нас ограничение на ширину полосы маленькое. То есть полоса маленькая и меньше объектов туда попадет и разделяющая полоса имеет больше свободы "крутиться". То есть нам не важно насколько большая полоса, нам важно чтобы как можно меньше объектов имели #M < 1$
2. Если он маленький, то всё будет наоборот

Мы можем использовать другую функцию скалярного произведения из другого гильбертова пространства. Чтобы это сделать мы переходим в другое пространство и вводим некоторую функцию ядра. Новое пространство мы можем явно не задавать мы задаем какое-нибудь ядро, которому будет соответствовать какое-нибудь гильбертово пространство. И более того, так как мы поменяли ядро, по сути, подменой ядра мы поменяли скалярное произведение в пространстве, а из скалярного произведения мы можем индуцировать норму, то есть по сути мы поменяли наше пространство - там теперь стала другая норма.

То есть если мы это сделали, то мы неявно нелинейно преобразовали исходное пространство признаков.

То есть у нас всё равно будет "линейная" гиперплоскость в новом гильбертовом пространстве, но в исходом - будет нелинейной.

Таким образом, подобрав правильное ядро мы спрямляем наше пространство. То есть переводи исходное пространство признаков в такое пространство, в котором выборка линейно разделима.

Однако проблема является в поиске нужного ядра. 

## PCA

Зачастую у нас огромное число признаков и из-за этого появляются следующие проблемы:
1. Время на вычисления линейно (как минимум) растет при увеличении размерности пространства
2. Проклятие размерности - количество равноудаленных точек растет очень быстро при увеличении размерности пространства
Поэтому в многомерных пространствах у нас появляются пробелемы с метрическими алгоритмами.

Соответственно появляется необходимость в снижении размерности.
В простейшем случае мы будем факторизовать матрицу плана (feature matrix) несколькими матрицами так, чтобы терялось как можно меньше информации.
Таких разложений есть много. 

Так как разложение по собственным векторам и числам можно применять только к квадратной матрице (там помимо этого есть еще свойства (должна быть неотрицательной квадратичной формой )), а наша design matrix в основном не квадратная, то будем применять сингулярное разложение.
Для этого рассматривается матрица $X^TX$ (она уже будет удовлетворять всем условия собственного разложения), у нее мы находим собственные числа и вектора и формируем 3 матрицы. Собственные вектора мы нормируем и тогда у нас получается разложение на 3 матрицы: $X = U \Sigma V^T$, где $U$ и $V^T$ орногональные матрицы (их определитель равен 1, а значит они задают повороты), а $\Sigma$ - матрица с сингулярными числами на диагонали, которые задают степень растяжения в направлении соответствующих векторов. 
Соответсвенно мы можем отсортировать все векторы и сингулярные числа в этих матрицах по невозрастанию сингялрных чисел на диагонали $\Sigma$. Далее можно применить теорему Экхарта-Янга, которая утверждает, что truncated SVD дает лучшую (в смысле нормы фробениуса) low-rank аппроксимацию матрицы $A$.
Направления собственных чисел соответствуют максимальной дисперсии в исходном признаковом пространстве.

Таким образом мы можем снизить число размерности признаков с максимальным сохранением дисперсии в новом базисе. 

Итого, метод главных компонент ищет направление наибольшей дисперсии. А главные компоненты - направления наибольшей дисперсии в наших данных.
Таким образом мы избавляемся от low-level variance признаков, которые не особо информативны.

PCA - это проецирование на *линейное* подпространство, а значит беря первые k компонент и проецируя их мы учитываем только линейные взаимосвязи между ними и теряем нелинейные. 

Выборку обязательно нужно отнормировать перед использованием PCA, так как чем больше шкала признака тем будет больше дисперсия по ней и это будет приводить к неправильным результатам.

Логистическая регрессия отличается от SVM минимизирующим функционалом:
1. SVM: $max(0; 1-M) + ||\omega||^2_2$  
2. Logistic regression: $-\Sum_i^2 p_i log(q_i) + ||\omega||^2_2$ = математическое ожидание negative log likelihood = cross-entropy