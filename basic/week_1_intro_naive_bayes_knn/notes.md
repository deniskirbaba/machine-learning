При решении задачи с использованием машинного обучения не нужно сразу выбирать модель. 
Использование той или иной модели должно основываться на:
1. Тип решаемой задачи (классификация, регрессия...)
2. Функционал качества (функция потерь) - функция, которую мы будем оптимизировать и которая будет показателем качества работы модели
3. Данные - их количество, их тип и т.д.
Только после анализа этих пунктов следует выбирать модель.

MLE (Maximum likelihood estimation) - такое значение параметров случайной величины, при которой мы наблюдаем наибольшее правдоподобие при проведенной серии экспериментов.
Стоит помнить, что функция правдоподобия не является функцией вероятности (и может не удовлетворять её свойствам). Так как у функции правдоподобия аргумент - параметры случайной величины (а не сама случайная величина).

Наивное предположение в Naive Bayes Classifier - features are independent.

Улучшением KNN является Weighted KNN, добавление весов позволяет добавить функцию при расчете принадлежности объекта к определенному классу. Например, в качестве функции можно использовать функцию расстояния или функцию веса класса.

# Что значит, что датасет порожден каким-то распределением с параметром $\theta$

If a dataset is generated by a distribution with parameter $\theta $, it means that the observed data points in the dataset are assumed to be samples drawn from a probability distribution characterized by this parameter $\theta $.

#### 1. **Distribution Parameterization**
Let $ \mathcal{D} = \{ x_1, x_2, \dots, x_n \} $ represent the dataset, where $ x_i $ is the $ i $-th data point, and $ n $ is the total number of data points. The distribution of these data points is parameterized by $ \theta $, meaning the probability of observing a data point $ x $ is determined by a probability distribution $ p(x \mid \theta) $.

Formally:
$$
x_1, x_2, \dots, x_n \sim p(x \mid \theta)
$$
where $ p(x \mid \theta) $ is the probability density function (for continuous distributions) or the probability mass function (for discrete distributions), and $ \theta $ is a parameter (or a set of parameters) that defines the shape of the distribution.

#### 2. **i.i.d. Assumption (Independent and Identically Distributed)**
In many cases, it is assumed that the data points are **independent and identically distributed (i.i.d.)**. This means that each data point $ x_i $ is drawn independently from the same distribution. Mathematically, this can be written as:

$$
x_i \sim p(x \mid \theta), \quad \text{for } i = 1, 2, \dots, n
$$
where each $ x_i $ is an independent sample from the same distribution $ p(x \mid \theta) $.

#### 3. **Joint Probability of the Dataset**
Under the i.i.d. assumption, the joint probability (or likelihood) of observing the entire dataset $ \mathcal{D} = \{ x_1, x_2, \dots, x_n \} $ given the parameter $ \theta $ is the product of the individual probabilities:

$$
p(x_1, x_2, \dots, x_n \mid \theta) = \prod_{i=1}^{n} p(x_i \mid \theta)
$$

This is also referred to as the **likelihood function** when viewed as a function of $ \theta $:

$$
L(\theta \mid x_1, x_2, \dots, x_n) = \prod_{i=1}^{n} p(x_i \mid \theta)
$$

#### 4. **Examples of Distributions with Parameters $ \theta $**

- **Normal distribution**: If the dataset is generated from a normal distribution, the parameter $ \theta = (\mu, \sigma^2) $ represents the mean $ \mu $ and variance $ \sigma^2 $:
  $$
  x_i \sim \mathcal{N}(\mu, \sigma^2)
  $$
  The probability density function is:
  $$
  p(x \mid \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right)
  $$

- **Bernoulli distribution**: If the dataset is generated by a Bernoulli distribution, the parameter $ \theta = p $ represents the probability of success:
  $$
  x_i \sim \text{Bernoulli}(p)
  $$
  The probability mass function is:
  $$
  p(x \mid p) = p^x (1 - p)^{1 - x}, \quad x \in \{0, 1\}
  $$