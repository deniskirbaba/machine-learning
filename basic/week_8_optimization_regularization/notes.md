# Optimization
## Moving average
Cumulative average: $CA_{k+1} = (x_{k+1} + k * CA_k)/(k + 1)$.

Simple moving average - unweighted mean of previous k data-points.
При расчете следующего значения - мы убираем самое старое значение и добавляем новое и делим эти слагаемые на k.

Exponential smoothing (exponential moving average) - is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time.
$ s_t = \alpha x_t + (1 - \alpha) s_{t-1}$
То есть, чем дальше у нас было наблюдение, тем меньше его влияние на величину и это влияние экспоненциально угасает.

## Momentum SGD
Имеет проблему, что если мы накопили слишком большой momentum, то нам нужно много времени чтобы его загасить.  
Еще бОльшая проблема в том, что если у нас слишком большой momentum, то тогда тот градиент в точке в которой мы вычислили вообще говоря будет сильно отличаться от той точки из которой мы шагаем - эту проблему решает Nesterov momentum - тут мы сначало сдвигаемся по накопленному градиенту, а потом уже в этой точке оцениваем градиент.

Также из-за того что мы накапливаем градиенты, то шум (который в среднем даёт ноль) будет также меньше в нашей оценке градиента - что позитивно сказывается на сходимости.

Также следует помнить, что momentum значения нужно где-то хранить - а значит это будет занимать память.

По поводу увеличения числа операций на каждом шаге спуска - это tradeoff - делаем больше вычислений, но тогда у нас шаг более оптимальный и мы за меньшее число шагов куда-то сойдемся.

Также имеем проблему, что в разных направлениях у нас градиенты разные (а learning_rate один) - мы хотели бы его (learning_rate) отнормировать так, что в тех направлениях в которых градиенты маленькие, learning_rate будет большой и наоборот.

Это делает Adagrad (тут есть проблема в том, что у нас градиенты будут затухать со временем), и его улучшение RMSProp (тут эта проблема решена с помощью экспоненциального сглаживания).

Adam = momentum + RMSProp + корректировка

Adam хорошо работает только для маленьких моделей, так как в нем потребление памяти x3. Поэтому зачастую в каких-то SOTA CV моделях можно увидеть SGD.

Так как в целом, мы вводили все эти оптимизаторы для того, чтобы улучшить сходимость, а так как все эти методы используют доп память, то мы просто можно увеличить размер батча и получить схожий (или даже лучше) результат.

3e-4 - is the best learning rate for Adam (karpathy) - но это для сложных моделей

Обычно можно начинать с learning_rate порядка 1e-1, 1e-2. И её можно уменьшать. В торче есть специальные schedulers. Ошибка модели вообще говоря обычно уменьшается как перевернутый логарифм.

Самое нормальное решение - уменьшать learning rate при выходе на плато.

# Regularization
Регуляризация в общем смысле - это внесение любых ограничений на решаемую задачу в некотором математическом смысле.  
Например - меняем функцию потерь - добавляем L1 регуляризация - появляются новые градиенты от второго кусочка этой функции потерь и они на что-то влияют.
L1-регуляризация - приводит к занулению некоторых весов.
L2-регуляризация - приводит к тому, что амплитуды весов примерно одинаковы.

То есть регуляризация - это не какой-то штраф - это любые ограничения, изменения нашей модели в связи с какими-то здравыми априорными предположениями.

Но точно также мы можем поменять и архитектуры модели, чтобы она больше подходила под конкретную задачу  (например рекурретные сети хорошо справляются с последовательными данными, сверточные сети хорошо справляются с данными у которых есть локальность). 

Всегда когда мы строим модель необходимо настраивать пайплайн валидации для того чтобы видеть как она себя ведет на отложенных данных - что позволит избежать переобучения.

Это еще больше необходимо на сложных моделях, так как если большая модель переобучиться под train, то мы потратим много энергию, время и силы в пустую.

Хороший пайплайн валидации - это создать валидационную выборку с одинаковым балансом классов (стратификация - попытка разбить на случайные подвыборки, но с сохранением требуемых статистик), совпадающим распределением классов (совпадающим со всем датасетом) и т.д.

Нормировать данные нужно всегда (почти). Так как иначе это может приводить к неустойчивости (взрыв градиентов, а так же так как около нуля точек больше у floating-point)

## Инициализация весов
Инициализировать все веса нулями (или любыми одинаковыми значениями) - плохо, так как у нас все элементы одинаковым образом вкладываются в выход, а значит все градиенты на них будут одинаковыми, так как у них у всех один и тот же вклад. Получается что у нас вырождается всё пространство в 1 нейрон.

Инициализация случайными малыми числами (которые пришли из одного и того же распределения с конечной дисперсией). Это тоже плохо работает. Так как в этом случае получится, что дисперсия линейной комбинации входа с весами будет умножаться на n - число входов. Так как у нас дисперсия суммы случайных величин это сумма дисперсии случайных величин. А так как у нас будет расти дисперсия, то с увеличением слоев у нас будет огромный сигнал, а с ним и шум и поэтому уже будет сложно вытащить верный сигнал из этого шума - будут неустойчивые первые шаги.

ПОэтому используются нормированные случайные числа для инициализации. Тогда у нас дисперсия будет сохраняться, например брать инициализацию весов из распределения с дисперсией 1/n.  

## Batch normalization
https://arxiv.org/abs/1502.03167
BatchNorm alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian (not always gaussian, but at least with mean=0 and std=1) distribution at the beginning of the training.

Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.

Проблема (covariate shift):
* Рассмотрим нейрон в любом скрытом слое, кромер первого
* На каждой итерации мы настраиваем его веса с помощью антиградиента функции потерь
* Однако мы также настраиваем веса в предыдущем слое, а значит изменяем и вход на следующий слой (то есть обновив данные на предыдущем слое, мы поменяли распределение данных которое приходит на следующий слой - получается второй слой обновился на распределение данных, которое более не актуально)
* Таким образом, теперь нам необходимо перенастроить опять веса в этом слое, из-за того что поменялся вход

This problem slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.

А мы всегда хотим, чтобы наши данные были i.i.d., а из-за этой проблемы у нас получается, что какому-то слою приходят данные из разных распределений.

Чтобы избежать данной проблемы используется BatchNorm. Считаем во время training среднее выборочное и выборочную дисперсию и нормализуем данные. Считаем эти статистики по каждому отдельному латентному признаку! Также мы обновляем эти статистики на каждом шаге training с помощью экспоненциального среднего. Тогда на инференсе мы будем использовать эти накопленные статистики. 

Обычно batchnorm ставят на каждый 3-ий слой.

Обычно после batchnorm предлагают вставлять также линейное преобразование:
$y = \gamma x + \beta$ (channel-wise scaling), где гамма и бета - обучаемые параметры. 
То есть мы дали возможность преобразованию (batchnorm) быть тождественным. То есть благодаря этому, преобразование (нормализации) может "отключиться" автоматически (если модель выучит gamma=выборочной дисперсии, а beta=выборочное среднее), если так будет оптимально для минимизации функции потерь.

То есть на уровне архитектуры модели можно закладывать некоторые свойства тех отображений с которыми мы работаем.

BatchNorm тоже относят к методам регуляризации, так как с его использованием (с его добавлением в архитектуру нейронной сети) мы предполагаем что промежуточные представления данных должны иметь нулевое среднее и единичную дисперсию.

### from https://www.youtube.com/watch?v=nUUqwaxLnWs&list=WL&index=8&t=11s
Covariate shift - проблема, когда у нас меняются распределение (или его параметры) на входе в модель (или в следующий слой нейронной сети). Из-за этого модель (или последующие слои нейронной сети) требуется переобучать, чтобы подстроиться под новые данные.
BatchNorm как раз таки уменьшает ограничивает изменение распределения (делает так, что независимо от того, как изменяются данные, у них будет mean=0, var=1), позволяя нам быстрее переобучиться на новые данные -> в нейронной сети мы можем использовать бОльший learning_rate и, соответственно, быстрее пройдет процесс обучения.

BatchNorm имеет также еще один эффект:
* так как у нас среднее и дисперсия считаются для батча, то это означает, что мы вносим немного шума в этот батч (а значит и в значения весов в этом слое), а значит это действует как регуляризация (подобно DropOut где мы добавляли шум в качестве умножения весов на 0 с некоторой вероятностью). Однако эффект от такой регуляризации очень маленький (из-за того что шум маленький)

## Layer normalization
Layer normalization normalizes the inputs across the feature dimension for each individual sample, which means it computes the mean and variance for each sample individually.

Advantages:
* Independent of Batch Size: It can be used effectively even with very small batch sizes or in settings where the batch size varies, such as in online learning or reinforcement learning.
* Effective for Sequential Data: It works well with RNNs and transformers, where normalization across the time or feature dimensions is more natural.
Disadvantages:
* Potentially Less Effective for Certain Architectures: In some scenarios, batch normalization may outperform layer normalization, especially in convolutional neural networks (CNNs) where spatial dependencies are crucial

## Dropout
Техника регуляризации которая широко используется.

Позволяет предотвратить переобучение на какой-то признак (латентный или нет).

Тут мы умножаем наше промежуточное признаковое описание на булеву маску, которая имеет бернуллевское распределение с параметром p (вероятность занулить признак). И потом умножаем признаки (получается умножаем оставшиеся признаки) на величину 1/(1-p) для того чтобы keeping mean values of features close to mean values which will be in test mode.

То есть dropout по сути позволяет выучить множество подсетей (так как мы зануляем каждый раз случайно), а потом при инференсе мы берем ансамбль этих подсетей.

То есть мы случайным образом "отключаем" часть сети каждый раз (отключает как forward pass, так и при backward pass по ним не будут идти градиенты).

Таким образом DropOut позволяет нам убедиться в том, что у нас все части сети будут обучены.

ОДНАКО, у нас получается (например при p=0.5), что на обучении у нас всегда используется только половина признаков из всех признаков, а значит эффективная обобщающая способность такой модели будет как у модели без дропаута но с числом признаков в 2 раза меньше. 

Таким образом, мы уменьшаем общую обобщающую способность модели, но увеличиваем её устойчивость.

Дропаут также успешно использует в случае зашумленных данных - вставляют после 1 слоя с p=0.8, тогда модель учится предсказывать только по кусочку контекста (в случае с bag of words входом).

### Word Dropout
Это техника аугментации, где часть слов, которые нам известны, случайным образом заменяются специальным токеном - undefined.

## Data augmentation
Это внесение каких-то изменений в данные.

Аугментация - способ регуляризации. 

Однако все преобразования, которые применяются к данным должны быть адекватны относительно решаемой задачи.
То есть все преобразования должны сохранять то распределение, откуда пришли данные 

Аугментация позволяет сделать модель устойчивой к используемым преобразованиям

## Вывод
Регуляризация - техника по внесению предположений решаемой задачи в тот математический аппарат, которым мы эту задачу решаем. 

## Practice по dataloaders
Обычно у нас данные лежат не в оперативной памяти, а где-то на диске, в интернете и т.д. И загрузить их полностью в RAM (оперативку) бывает невозможно из-за их объема.
Для этого люди придумали в pytorch класс Dataset.

https://pytorch.org/tutorials/beginner/basics/data_tutorial.html
Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.

Обычно мы наследуемся от класса Dataset и имплементируем 3 метода:
1. __init__ - тут можем указать путь до наших данных
2. __len__
3. __getitem__ - тут мы реализуем логику загрузки данных и их преобразования (если они требуются)

Класс Dataset интерфейсами "повязан" с классом Dataloader.
Dataloader - класс который умеет эффективно загружать данные батчами.

В торче есть класс Compose (аналог Pipeline в sklearn) - он берет несколько преобразований и делает их композицию.
Для этого нужно в каждом классе преобразования реализовать 3 метода:
1. __init__
2. __call__
3. __repr__
А затем эти классы передать в Compose.

Разбить датасет на определенные части (валидационная, тестировочная) можно с помощью класса Subset.

## Архитектура нейронной сети
Есть байесовский и фишеровский подходы к статистике.  
Фишеровская статистика работает когда у нас количество оцениваемых параметров много меньше чем количество данных.
Однако в глубоком обучении у нас наоборот колчиество параметров много больше чем колчество данных.

Однако нейронные сети не подчиняются этому закону, так как они дополнительно учитывают внутреннюю структуру данных (свертка, attention, рекуррентность и т.д.) -> получается что количество параметров будет много меньше, чем количество данных.

Как понять что количество параметров слишком большое для решаемой задачи?
 - По валидационной выборке. Если мы видим что у нас модель сразу же (начиная с первых эпох) переобучается (валидационная ошибка увеличивается), то у нас overparametrized модель - надо её упрощать или регуляризировать.

Подбирать правильную архитектуру сети нужно по:
* характерный размер выборки
* характерная архитектура для данной задачи
* изучаем статьи и бенчмарки от других коллективов
* повторяем
Таким образом, получим хорошую инициализацию для архитектуры для конкретной задачи. Иначе можно потратить большое количество времени на поиск подходящей инициализации для архитектуры сети.

## Сохранение моделей
Хранить модель в runtime плохая идея - нужно делать чекпоинты.

В PyTorch у модели есть state_dict - те параметры которые в ней хранятся (это словать из всех параметров).
Также важно сохранять state_dict у оптимизатора (если мы хотим потом модель дообучать), так как у оптимизатора тоже есть какие-то накопленные статистики, которые требуется сохранить. 