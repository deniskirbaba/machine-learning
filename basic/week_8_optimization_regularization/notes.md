# Optimization

## Moving average

Cumulative average: $CA_{k+1} = (x_{k+1} + k \cdot CA_k)/(k + 1)$.

Simple moving average - unweighted mean of previous $k$ data-points.
При расчете следующего значения - мы убираем самое старое значение и добавляем новое и делим эти слагаемые на $k$.

Exponential smoothing (exponential moving average) - is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time.
$ s_t = \alpha x_t + (1 - \alpha) s_{t-1}$
То есть, чем дальше у нас было наблюдение, тем меньше его влияние на величину и это влияние экспоненциально угасает.

## Gradient optimization methods

### Momentum

Имеет проблему, что если мы накопили слишком большой momentum, то нам нужно много времени чтобы его загасить.  
Еще бОльшая проблема в том, что если у нас слишком большой momentum, то тогда тот градиент в точке в которой мы вычислили вообще говоря будет сильно отличаться от той точки из которой мы шагаем - эту проблему решает Nesterov momentum - тут мы сначало сдвигаемся по накопленному градиенту, а потом уже в этой точке оцениваем градиент.

Из плюсов можно выделить то, что из-за того что мы накапливаем градиенты, то шум (который в среднем даёт ноль) будет также меньше в нашей оценке градиента - что позитивно сказывается на сходимости.

Также следует помнить, что momentum значения нужно где-то хранить - а значит это будет занимать память.

По поводу увеличения числа операций на каждом шаге спуска - это tradeoff - делаем больше вычислений, но тогда у нас шаг более оптимальный и мы за меньшее число шагов куда-то сойдемся.

### Cache

Также имеем проблему, что в разных направлениях у нас градиенты разные (а learning_rate один) - мы хотели бы его (learning_rate) отнормировать так, что в тех направлениях в которых градиенты маленькие, learning_rate будет большой и наоборот.

Это делает Adagrad (тут есть проблема в том, что у нас градиенты будут затухать со временем), и его улучшение RMSProp (тут эта проблема решена с помощью экспоненциального сглаживания).

### Adam = momentum + cache

Adam = momentum + RMSProp + корректировка

Adam хорошо работает только для маленьких моделей, так как в нем потребление памяти x3. Поэтому зачастую в каких-то SOTA CV моделях можно увидеть SGD.

Так как в целом, мы вводили все эти оптимизаторы для того, чтобы улучшить сходимость, а так как все эти методы используют доп память, то мы просто можно увеличить размер батча и получить схожий (или даже лучше) результат.

3e-4 - is the best learning rate for Adam (karpathy) - но это для сложных моделей

Обычно можно начинать с learning_rate порядка 1e-1, 1e-2. И её можно уменьшать. В торче есть специальные schedulers. Ошибка модели вообще говоря обычно уменьшается как перевернутый логарифм.

Самое нормальное решение - уменьшать learning rate при выходе на плато.

## Инициализация весов в нейронных сетях

### Почему плохо инициализировать веса нулями?

Инициализация всех весов нулями (или одинаковыми значениями) приводит к проблеме **симметрии**. Все нейроны в каждом слое будут вычислять одинаковые значения на каждом шаге и получать одинаковые градиенты во время обратного распространения. Это означает, что они будут обновляться одинаково, что сводит многослойную нейронную сеть к однослойной — все нейроны в слое становятся эквивалентны, и сеть теряет свою выразительную мощность. Поэтому такая инициализация ведет к **вырождению** модели и неспособности обучаться.

### Почему случайные малые веса из одинакового распределения могут привести к проблемам?

Инициализация весов случайными малыми числами, взятыми из одного и того же распределения с конечной дисперсией (например, нормальное распределение), тоже может вызвать проблемы, особенно в глубоких сетях. Когда нейрон получает на вход линейную комбинацию сигналов с весами, дисперсия выходного сигнала увеличивается пропорционально числу входов $n$, поскольку дисперсия суммы случайных величин равна сумме их дисперсий. 

Это ведет к тому, что с увеличением числа слоев дисперсия сигналов начинает расти, что может привести к следующему:

- **Затухание градиентов** — в сигналах на обратном проходе будут очень маленькие значения, из-за чего обновления весов будут крайне малы, и сеть будет обучаться очень медленно.
- **Взрыв градиентов** — слишком большие значения сигналов могут привести к нестабильному обучению, когда обновления весов станут слишком большими, что приведет к резким колебаниям или "взрыву" градиентов.

Таким образом, случайная инициализация весов без учета структуры сети может сделать обучение нестабильным.

### Правильные стратегии инициализации

Для решения этой проблемы применяют инициализацию весов так, чтобы дисперсия сохранялась на протяжении всех слоев сети. Это достигается нормированием весов с учётом количества входов и/или выходов нейронов. Например:

- **Xavier (Glorot) инициализация**: веса инициализируются из равномерного или нормального распределения с дисперсией, равной $1/n_{\text{входов}}$ (или $1/\sqrt{n_{\text{входов}}}$). Это помогает контролировать дисперсию сигналов через каждый слой, что улучшает стабильность обучения.
  
- **He инициализация**: для сетей с нелинейностями типа ReLU рекомендуется использовать инициализацию весов с дисперсией, зависящей от числа входов $n_{\text{входов}}$. Например, для нормального распределения применяют дисперсию $2/n_{\text{входов}}$, что предотвращает затухание градиентов и ускоряет обучение.

# Регуляризация в нейронных сетях

**Регуляризация** — это процесс внесения ограничений или дополнительных условий в модель для предотвращения переобучения и улучшения её обобщающей способности. Эти ограничения могут быть введены различными способами, влияющими как на структуру модели, так и на функцию потерь. Основная цель регуляризации — заставить модель "учиться" более простым зависимостям, которые лучше обобщаются на новые данные.

## Примеры регуляризации через изменение функции потерь

Часто регуляризация реализуется путем изменения функции потерь. Это может быть сделано добавлением специальных штрафов на параметры модели (например, веса). Примером таких методов являются:

- **L1-регуляризация (Lasso)**: добавляется сумма абсолютных значений весов к функции потерь. Это приводит к тому, что некоторые веса становятся нулевыми, что создает разреженную модель. Такой метод полезен для отбора признаков, поскольку он "обнуляет" незначимые веса, что уменьшает сложность модели.

- **L2-регуляризация (Ridge)**: добавляется сумма квадратов весов к функции потерь. Этот метод способствует тому, чтобы веса стремились к небольшим значениям, но не к нулю. Он снижает разброс весов, приводя их к приблизительно одинаковой амплитуде. L2-регуляризация хорошо справляется с переобучением за счет того, что модель "наказывается" за слишком большие веса.

## Широкое понимание регуляризации

Регуляризация — это не только штрафы на веса (например, L1 и L2). Любое изменение модели, которое вносит априорные знания о задаче и помогает улучшить её обобщающую способность, можно рассматривать как форму регуляризации. 

Примером такого подхода может быть изменение **архитектуры модели**. Например:
- **Рекуррентные нейронные сети (RNN)** — подходят для задач с последовательными данными (например, временные ряды, текст), поскольку могут запоминать контекст прошлых данных.
- **Сверточные нейронные сети (CNN)** — эффективно работают с изображениями и данными с локальной структурой благодаря сверткам, которые фокусируются на небольших локальных областях.

## Регуляризация через настройку пайплайна валидации

Важной частью регуляризации является правильная настройка процесса валидации. Обучение на тренировочных данных без качественной оценки на отложенных данных может привести к **переобучению**. Валидация позволяет отслеживать, как модель обобщает, и при необходимости корректировать гиперпараметры или архитектуру. 

При построении пайплайна валидации важно учитывать следующее:
- **Стратификация** — разбиение данных на обучающую и валидационную выборки таким образом, чтобы сохранялся баланс классов и ключевые статистики выборки совпадали с общим распределением данных. Это особенно важно в задачах классификации.
- **Правильное распределение данных** — выборки для тренировки и валидации должны иметь схожие распределения классов и признаков, чтобы оценка модели была объективной.

## Нормализация данных

Нормализация данных — важный шаг в большинстве случаев. Ненормализованные данные могут привести к **взрыву градиентов**, особенно если различные признаки имеют разные масштабы. Это усложняет процесс обучения нейронной сети, поскольку большие значения входных данных могут вызвать нестабильность в процессе оптимизации.

Также нормализация важна из-за особенностей представления чисел с плавающей точкой (floating-point). Вблизи нуля плотность представления чисел выше, что позволяет более точно обрабатывать данные, когда они нормированы.

Normalizing features (or feature scaling) can significantly speed up learning in machine learning algorithms for several reasons:

### 1. **Gradient Descent Convergence**:
   Many learning algorithms, like gradient descent, use optimization techniques that are sensitive to the scale of the features. If features have very different ranges, the gradients will be skewed, leading to inefficient updates to the model parameters.

   - **Without normalization**: Features with larger values can dominate the gradient calculations, causing the algorithm to take very small steps for features with smaller values. As a result, convergence can be slow or the algorithm may not converge at all.
   - **With normalization**: When all features are on a similar scale, the optimization algorithm updates parameters more uniformly, allowing it to converge faster.

### 2. **Improved Condition Number**:
   The **condition number** of a problem refers to how sensitive the output is to small changes in the input. If features are on different scales, the condition number becomes large, making the optimization problem ill-conditioned. This can cause slower convergence in iterative algorithms like gradient descent.
   
   - Normalization reduces the condition number, leading to a well-conditioned problem that can be solved more efficiently.

### 3. **Prevents Bias Toward Large-Scale Features**:
   If features are not normalized, machine learning algorithms that rely on distance metrics (e.g., k-NN, SVMs, and algorithms using regularization like Ridge or Lasso regression) will give more weight to features with larger numerical values, even if they are not inherently more important. This skews the learning process.
   
   - By normalizing, each feature contributes equally, ensuring the model learns in a balanced way.

### 4. **Eases Regularization**:
   Regularization techniques (e.g., L1, L2 regularization) penalize large parameter values to avoid overfitting. If features are not normalized, the model may overly penalize features with large values, making it harder for the model to generalize well.
   
   - Normalizing features ensures that regularization terms work evenly across all features, helping in better generalization and faster convergence.

### 5. **Enhanced Numerical Stability**:
   Scaling features can also improve **numerical stability**, especially for algorithms like matrix factorization, support vector machines (SVM), or neural networks, where unscaled features can lead to very large values in computations (e.g., dot products), causing overflow or underflow.

### 6. **Neural Networks Activation Functions**:
   In neural networks, activation functions like the sigmoid or tanh saturate (i.e., their gradients become very small) for large input values. If features are not normalized, the activations may be far from the non-linear regions of the functions, slowing down learning due to vanishing gradients.
   
   - Normalization keeps inputs within a range that helps keep the gradients of the activation functions in a range where learning is faster.

## Batch normalization

[Original paper](https://arxiv.org/abs/1502.03167)

BatchNorm alleviates a lot of headaches with properly initializing neural networks by explicitly forcing the activations throughout a network to take on a unit gaussian (not always gaussian, but at least with mean=0 and std=1) distribution at the beginning of the training.

Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout.

Проблема (covariate shift):
* Рассмотрим нейрон в любом скрытом слое, кромер первого
* На каждой итерации мы настраиваем его веса с помощью антиградиента функции потерь
* Однако мы также настраиваем веса в предыдущем слое, а значит изменяем и вход на следующий слой (то есть обновив данные на предыдущем слое, мы поменяли распределение данных которое приходит на следующий слой - получается второй слой обновился на распределение данных, которое более не актуально)
* Таким образом, теперь нам необходимо перенастроить опять веса в этом слое, из-за того что поменялся вход

This problem slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.

А мы всегда хотим, чтобы наши данные были i.i.d., а из-за этой проблемы у нас получается, что какому-то слою приходят данные из разных распределений.

Чтобы избежать данной проблемы используется BatchNorm. Считаем во время training среднее выборочное и выборочную дисперсию и нормализуем данные. Считаем эти статистики по каждому отдельному латентному признаку! Также мы обновляем эти статистики на каждом шаге training с помощью экспоненциального среднего. Тогда на инференсе мы будем использовать эти накопленные статистики. 

Обычно batchnorm ставят на каждый 3-ий слой.

Обычно после batchnorm предлагают вставлять также линейное преобразование:  
$$y = \gamma x + \beta$$
т.н. channel-wise scaling, где гамма и бета - обучаемые параметры.   

То есть мы дали возможность преобразованию (batchnorm) быть тождественным. То есть благодаря этому, преобразование (нормализации) может "отключиться" автоматически (если модель выучит gamma=выборочной дисперсии, а beta=выборочное среднее), если так будет оптимально для минимизации функции потерь.

То есть на уровне архитектуры модели можно закладывать некоторые свойства тех отображений с которыми мы работаем.

BatchNorm тоже относят к методам регуляризации, так как с его использованием (с его добавлением в архитектуру нейронной сети) мы предполагаем что промежуточные представления данных должны иметь нулевое среднее и единичную дисперсию.

### [From Anrew Ng video](https://www.youtube.com/watch?v=nUUqwaxLnWs&list=WL&index=8&t=11s)

Covariate shift - проблема, когда у нас меняются распределение (или его параметры) на входе в модель (или в следующий слой нейронной сети). Из-за этого модель (или последующие слои нейронной сети) требуется переобучать, чтобы подстроиться под новые данные.  

BatchNorm как раз таки ограничивает изменение распределения (делает так, что независимо от того, как изменяются данные, у них будет mean=0, var=1), позволяя нам быстрее переобучиться на новые данные -> в нейронной сети мы можем использовать бОльший learning_rate и, соответственно, быстрее пройдет процесс обучения.

BatchNorm имеет также еще один эффект:
* так как у нас среднее и дисперсия считаются для батча, то это означает, что мы вносим немного шума в этот батч (а значит и в значения весов в этом слое), а значит это действует как регуляризация (подобно DropOut где мы добавляли шум в качестве умножения весов на 0 с некоторой вероятностью). Однако эффект от такой регуляризации очень маленький (из-за того что шум маленький)

## Layer normalization

Layer normalization normalizes the inputs across the feature dimension for each individual sample, which means it computes the mean and variance for each sample individually.

Advantages:
* Independent of Batch Size: It can be used effectively even with very small batch sizes or in settings where the batch size varies, such as in online learning or reinforcement learning.
* Effective for Sequential Data: It works well with RNNs and transformers, where normalization across the time or feature dimensions is more natural.  

Disadvantages:
* Potentially Less Effective for Certain Architectures: In some scenarios, batch normalization may outperform layer normalization, especially in convolutional neural networks (CNNs) where spatial dependencies are crucial

## Dropout

Техника регуляризации которая широко используется.

Позволяет предотвратить переобучение на какой-то признак (латентный или нет).

Тут мы умножаем наше промежуточное признаковое описание на булеву маску, которая имеет бернуллевское распределение с параметром p (вероятность занулить признак). И потом умножаем признаки (получается умножаем оставшиеся признаки) на величину 1/(1-p) для того чтобы keeping mean values of features close to mean values which will be in test mode.

То есть dropout по сути позволяет выучить множество подсетей (так как мы зануляем каждый раз случайно), а потом при инференсе мы берем ансамбль этих подсетей.

То есть мы случайным образом "отключаем" часть сети каждый раз (отключает как forward pass, так и при backward pass по ним не будут идти градиенты).

Таким образом DropOut позволяет нам убедиться в том, что у нас все части сети будут обучены.

ОДНАКО, у нас получается (например при p=0.5), что на обучении у нас всегда используется только половина признаков из всех признаков, а значит эффективная обобщающая способность такой модели будет как у модели без дропаута но с числом признаков в 2 раза меньше. 

Таким образом, мы уменьшаем общую обобщающую способность модели, но увеличиваем её устойчивость.

Дропаут также успешно использует в случае зашумленных данных - вставляют после 1 слоя с p=0.8, тогда модель учится предсказывать только по кусочку контекста (в случае с bag of words входом).

### Word Dropout

Это техника аугментации, где часть слов, которые нам известны, случайным образом заменяются специальным токеном - undefined.

## Data augmentation

Это внесение каких-то изменений в данные.

Аугментация - способ регуляризации. 

Однако все преобразования, которые применяются к данным должны быть адекватны относительно решаемой задачи.
То есть все преобразования должны сохранять то распределение, откуда пришли данные 

Аугментация позволяет сделать модель устойчивой к используемым преобразованиям

## Вывод

Регуляризация - техника по внесению предположений решаемой задачи в тот математический аппарат, которым мы эту задачу решаем. 

# Архитектура нейронной сети

Есть байесовский и фишеровский подходы к статистике.  

Фишеровская статистика работает когда у нас количество оцениваемых параметров много меньше чем количество данных.
Однако в глубоком обучении у нас наоборот колчиество параметров много больше чем колчество данных.

Однако нейронные сети не подчиняются этому закону, так как они дополнительно учитывают внутреннюю структуру данных (свертка, attention, рекуррентность и т.д.) -> получается что количество параметров будет много меньше, чем количество данных.

Как понять что количество параметров слишком большое для решаемой задачи?
 - По валидационной выборке. Если мы видим что у нас модель сразу же (начиная с первых эпох) переобучается (валидационная ошибка увеличивается), то у нас overparametrized модель - надо её упрощать или регуляризировать.

Подбирать правильную архитектуру сети нужно по:
* характерный размер выборки
* характерная архитектура для данной задачи
* изучаем статьи и бенчмарки от других коллективов
* повторяем
Таким образом, получим хорошую инициализацию для архитектуры для конкретной задачи. Иначе можно потратить большое количество времени на поиск подходящей инициализации для архитектуры сети.

# Сохранение моделей

Хранить модель в runtime плохая идея - нужно делать чекпоинты.  

В PyTorch у модели есть state_dict - те параметры которые в ней хранятся (это словать из всех параметров).
Также важно сохранять state_dict у оптимизатора (если мы хотим потом модель дообучать), так как у оптимизатора тоже есть какие-то накопленные статистики, которые требуется сохранить. 

# Practice по dataloaders

Обычно у нас данные лежат не в оперативной памяти, а где-то на диске, в интернете и т.д. И загрузить их полностью в RAM (оперативку) бывает невозможно из-за их объема.  

Для этого люди придумали в pytorch класс Dataset.

[PyTorch tutorial for data](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)  

Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allow you to use pre-loaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.

Обычно мы наследуемся от класса Dataset и имплементируем 3 метода:
1. __init__ - тут можем указать путь до наших данных
2. __len__
3. __getitem__ - тут мы реализуем логику загрузки данных и их преобразования (если они требуются)

Класс Dataset интерфейсами "повязан" с классом Dataloader.  

Dataloader - класс который умеет эффективно загружать данные батчами.

В торче есть класс Compose (аналог Pipeline в sklearn) - он берет несколько преобразований и делает их композицию.
Для этого нужно в каждом классе преобразования реализовать 3 метода:
1. __init__
2. __call__
3. __repr__
А затем эти классы передать в Compose.

Разбить датасет на определенные части (валидационная, тестировочная) можно с помощью класса Subset.