# Notes from Yandex ML Handbook

## Introduction

Перед решением задачи необходимо определить к какому виду относится задача (классификация, регрессия, кластеризация...).

Далее выбираем метрику качества. Они имеют следующую иерархию:
1. Бизнес-метрики - показатели работы всей системы в целом, обычно они зависят не только от качества работы разработанной модели. Например, доход с торговой точки
2. Online-метрики - покатели работающей системы, которые высчитывают в реальном времени после внедрения модели. Например, медианное время проведенное в игре пользователем.
3. Ассесоры - показатели, оцененные специальными людьми - ассесорами (тестировщиками). Например, тестировщики оцениват качество ответа языковой модели. 
4. Offline-метрики - метрики, рассчитываемые при разработке модели, например используя исторические данные. Здесь мы используем классические метрики машинного обучения.

Также важно обратить внимание на данные, которые будут использованы при разработке модели. Являются ли они размеченными или нет, какого они вида (требуется ли feature engineering или representation learning), сколько у нас данных и какого они качества. Также отметим некоторые проболемы в данных, которые часто встречаются:
1. Пропуски
2. Выбросы
3. Ошибки разметки
4. Data drift

Далее выбираем модель и алгоритм её обучения. 

Важен и этап деплоймента модели. Необходимо эффективно запрограммировать модель и успешно встроить её в уже существующую систему. И подумать как и какие рассчитывать online-метрики. Также имеет смысл провести АБ-тестирование, то есть сравнение с предыдущей версией модели на случайно выбранных подмножествах пользователей или сессий. Если новая модель работает не очень здорово, должна быть возможность откатиться к старой.

После деплоймента модели важно продолжать дообучать или переобучать её при поступлении новых данных, а также мониторить качество. Существует concept drift — изменение зависимости между признаками и таргетом. Например, если вы делаете музыкальные рекомендации, вам нужно будет учитывать и появление новых треков, и изменение вкусов аудитории.

Data scientists обычно выбирают одни метрики для наблюдения за тренировкой модели и другой набор метрики при представлении результатов работы модели для работодателей/бизнеса и т.д. При этом выбор таких метрик должен основываться на следующих параметрах:
1. Насколько много выбросов в данных и как мы хотим их учитывать
2. Если разница между overforecating и unforecasting, и если она есть, каким образом мы будем её оценивать
3. Scale-dependent (MAE, MSE) и scale-independent (R2, NMAE). Scale-dependent изменяются при изменении величины данных, их удобно использовать для оценки общих ошибок в шкале единиц измерения таргета на каком-либо датасете, имеющим одну шкалу. Scale-independent используют нормализацию в каком-либо виде и поэтому не зависят от шкал данных, они удобны для сравнения работы модели на различных датасетах, имеющиъ различные шкалы.

При выборе метрик в задаче классификации следует опираться на следующее:
1. Распределение объектов по классам (сбалансированная выборка или нет). Для сбалансированных выборок хорошо подходит метрика accuracy, для несбалансированных presicion, recall, F1-score, area under the precision-recall curve (AUPRC)
2. Важность типа ошибок (ошибки 1-го рода или ошибки 2-го рода важнее). Исходя из важности можно смотреть на precision, recall или f1-score
3. Интерпретируемость для бизнеса
4. Цель модели. Важно ли нам создать обощенную модель (которая будет достаточно хорошо определять каждый класс) или же узко-направленную модель (которая будет хорошо определять только 1 конкретный класс). В зависимости от этого важность метрик будет различаться
5. Threshold sensibility. Метрика AUPRC не зависит от порога вероятности предсказания класса 
6. Количество классов (бинарная классификация или многоклассовая). В зависимости от этого confusion matrix, accuracy, precision, recall, f1-score могут быть адаптированы для многоклассовой классификации, используя macro, micro, weighted averages
7. Вычислительная сложность

Common Metrics for Classification
Binary Classification Metrics:
1. Accuracy: Proportion of correctly classified instances.
2. Precision: Proportion of true positive predictions among all positive predictions.
3. Recall (Sensitivity): Proportion of true positive predictions among all actual positives.
4. F1-Score: Harmonic mean of Precision and Recall, balancing the two.
5. AUROC (Area Under ROC Curve): Measures the ability of the model to distinguish between classes.
6. AUPRC (Area Under Precision-Recall Curve): Useful for imbalanced datasets.
7. Confusion Matrix: Provides counts of true positives, true negatives, false positives, and false negatives.
Multi-class Classification Metrics:
1. Accuracy: Overall correctness.
2. Precision, Recall, F1-Score: Can be computed for each class and averaged (macro, micro, weighted).
3. Confusion Matrix: Extended for multiple classes.
4. Logarithmic Loss (Log Loss): Measures the uncertainty of predictions.
5. Cohen’s Kappa: Measures the agreement between predicted and actual classes, adjusted for chance.

При адаптации метрик бинарной классификации к многоклассовой производится вычисление метрик для каждого класса и затем их усреднение по различному виду: macro, micro, weighted:
1. Macro - независимо рассчитываем метрику для каждого класса и затем берем среднее арифметическое по всем классам. macro_f1 = 1/N (sum of f1 for each class). Используем macro в случае несбалансированных классов. Хорошо подходит для получения представления об оценки по каждому классу, особенно если размеры классов одинаковы
2. Micro - представляет, что перед нами задача классификации и аггрегирует TP, FP, FN для всех классов. Используем micro в случае сбалансированных классов. Однако данный случай хорошо подходит для общей оценки, особенно когда размеры классов значительно отличаются.
3. Weighted - высчитывает независимо метрику для каждого класса и затем берем среднее с учетом количество объектов из каждого класса. Полезно при работе с несбалансированными наборами данных, чтобы убедиться, что большие классы не доминируют в метрике